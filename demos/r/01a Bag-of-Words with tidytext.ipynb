{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de3d350-f1e1-419d-bbc3-595f59c70bea",
   "metadata": {},
   "source": [
    "# Bag of Words in R with `tidytext`\n",
    "\n",
    "If you're using R, you almost certainly know about the [Tidyverse](https://www.tidyverse.org/) family of libraries.  (If not, go fix that, they're easily the \"killer app\" for R).  Part of the Tidyverse is `tidytext`: a library for transforming texts into \"one row per token per document\" format.\n",
    "\n",
    "If you're coming from other libraries or other NLP tools, this will be a very different way of thinking about your data.  Rather than performing operations on _documents,_ you'll be mostly performing operations on _words_.  There are some pros and cons to this approach, but at the end of the day they about even out to zero.  The biggest downside to this approach is that if you're using a train-test split (like we are here), you'll need to do a little extra work to make sure that certain transformations are applied uniformly to all your different splits.\n",
    "\n",
    "`tidytext` is great for exploratory analyses and anything where you're directly analysing _words._  The \"one row per token per document\" format is a very nice format for this kind of work.  Personally, I find it a little awkward for predictive modeling (at least compared to `text2vec`, which we'll see in the next notebook), but that's probably mostly a matter of familiarity.\n",
    "\n",
    "We'll use `tidytext` as part of a predictive modeling project.  We have a bunch of Amazon reviews, and we want to predict the number of stars from the review text.  We have a training split and a testing split for our data; we'll train a model using the training data, and evaluate the model on the testing data.  We'll use a Bernoulli Naive Bayes model to do the predictions--this is a super simple form of Naive Bayes that assumes all your features are either 1 or 0.  (this will let us take some shortcuts a bit later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7968d7-388c-41ff-8b8a-0263facc9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies---uncomment and run to install them\n",
    "# install.packages(\"dplyr\")      # if you don't know what dplyr is I can't help you\n",
    "# install.packages(\"magrittr\")   # pipes!\n",
    "# install.packages(\"naivebayes\") # Naive Bayes implementation\n",
    "# install.packages(\"SnowballC\")  # stemming\n",
    "# install.packages(\"tidytext\")   # text processing using Tidy data principles\n",
    "# install.packages(\"yardstick\")  # model metrics; part of the tidymodels suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e6c458-7bfe-4ffc-9f8b-85db93480354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "naivebayes 0.9.7 loaded\n",
      "\n",
      "For binary classification, the first factor level is assumed to be the event.\n",
      "Use the argument `event_level = \"second\"` to alter this as needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(magrittr)\n",
    "library(naivebayes)\n",
    "library(SnowballC)\n",
    "library(tidytext)\n",
    "library(yardstick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c1ab9b-fe5c-4e35-a366-b8b44efce118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t200000 obs. of  8 variables:\n",
      " $ review_id       : chr  \"en_0964290\" \"en_0690095\" \"en_0311558\" \"en_0044972\" ...\n",
      " $ product_id      : chr  \"product_en_0740675\" \"product_en_0440378\" \"product_en_0399702\" \"product_en_0444063\" ...\n",
      " $ reviewer_id     : chr  \"reviewer_en_0342986\" \"reviewer_en_0133349\" \"reviewer_en_0152034\" \"reviewer_en_0656967\" ...\n",
      " $ stars           : int  1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ review_body     : chr  \"Arrived broken. Manufacturer defect. Two of the legs of the base were not completely formed, so there was no wa\"| __truncated__ \"the cabinet dot were all detached from backing... got me\" \"I received my first order of this product and it was broke so I ordered it again. The second one was broke in m\"| __truncated__ \"This product is a piece of shit. Do not buy. Doesn't work, and then I try to call for customer support, it won'\"| __truncated__ ...\n",
      " $ review_title    : chr  \"I'll spend twice the amount of time boxing up the whole useless thing and send it back with a 1-star review ...\" \"Not use able\" \"The product is junk.\" \"Fucking waste of money\" ...\n",
      " $ language        : chr  \"en\" \"en\" \"en\" \"en\" ...\n",
      " $ product_category: chr  \"furniture\" \"home_improvement\" \"home\" \"wireless\" ...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train <- read.csv(\"../../data/train.csv\", stringsAsFactors = FALSE)\n",
    "test <- read.csv(\"../../data/test.csv\", stringsAsFactors = FALSE)\n",
    "str(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de4370-c068-4e48-a5c0-6481102cc1dd",
   "metadata": {},
   "source": [
    "`tidytext`'s main tool is the `unnest_tokens()` function.  It takes in a `data.frame` (or `data.frame`-compatible object like a `tibble`) along with the name of the column containing text, and the name of the column that should store the tokens after it's been tokenized.  This function will then run some basic preprocessing (mainly lowercasing the text and tokenizing it) and spit out a `data.frame` in the \"one row per token per document\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d67448-35d3-4992-8b8e-6a36d20a1d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>review_id</th><th scope=col>stars</th><th scope=col>token</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>en_0964290</td><td>1</td><td>arrived     </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>en_0964290</td><td>1</td><td>broken      </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>en_0964290</td><td>1</td><td>manufacturer</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>en_0964290</td><td>1</td><td>defect      </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>en_0964290</td><td>1</td><td>two         </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>en_0964290</td><td>1</td><td>of          </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>en_0964290</td><td>1</td><td>the         </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>en_0964290</td><td>1</td><td>legs        </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>en_0964290</td><td>1</td><td>of          </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>en_0964290</td><td>1</td><td>the         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & review\\_id & stars & token\\\\\n",
       "  & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & en\\_0964290 & 1 & arrived     \\\\\n",
       "\t2 & en\\_0964290 & 1 & broken      \\\\\n",
       "\t3 & en\\_0964290 & 1 & manufacturer\\\\\n",
       "\t4 & en\\_0964290 & 1 & defect      \\\\\n",
       "\t5 & en\\_0964290 & 1 & two         \\\\\n",
       "\t6 & en\\_0964290 & 1 & of          \\\\\n",
       "\t7 & en\\_0964290 & 1 & the         \\\\\n",
       "\t8 & en\\_0964290 & 1 & legs        \\\\\n",
       "\t9 & en\\_0964290 & 1 & of          \\\\\n",
       "\t10 & en\\_0964290 & 1 & the         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 10 × 3\n",
       "\n",
       "| <!--/--> | review_id &lt;chr&gt; | stars &lt;int&gt; | token &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | en_0964290 | 1 | arrived      |\n",
       "| 2 | en_0964290 | 1 | broken       |\n",
       "| 3 | en_0964290 | 1 | manufacturer |\n",
       "| 4 | en_0964290 | 1 | defect       |\n",
       "| 5 | en_0964290 | 1 | two          |\n",
       "| 6 | en_0964290 | 1 | of           |\n",
       "| 7 | en_0964290 | 1 | the          |\n",
       "| 8 | en_0964290 | 1 | legs         |\n",
       "| 9 | en_0964290 | 1 | of           |\n",
       "| 10 | en_0964290 | 1 | the          |\n",
       "\n"
      ],
      "text/plain": [
       "   review_id  stars token       \n",
       "1  en_0964290 1     arrived     \n",
       "2  en_0964290 1     broken      \n",
       "3  en_0964290 1     manufacturer\n",
       "4  en_0964290 1     defect      \n",
       "5  en_0964290 1     two         \n",
       "6  en_0964290 1     of          \n",
       "7  en_0964290 1     the         \n",
       "8  en_0964290 1     legs        \n",
       "9  en_0964290 1     of          \n",
       "10 en_0964290 1     the         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the bag-of-words representation in \"one token per row\" format.\n",
    "tokenized <- unnest_tokens(\n",
    "    train[, c(\"review_body\", \"review_id\", \"stars\")], # dataframe to process\n",
    "    token,      # name of the output column that stores the tokens\n",
    "    review_body # name of the input column that stores the text\n",
    ")\n",
    "\n",
    "# Note that all columns other than `token` and `review_body`, above,\n",
    "# are treated like document identifiers. (in this case, \"review_id\" \n",
    "# and \"stars\").  We won't actually use anything other than the\n",
    "# review_id column, so the stars column is just used to demonstrate\n",
    "# this behavior.\n",
    "head(tokenized, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb293e1-50b1-41da-92a9-0e25b627f4b1",
   "metadata": {},
   "source": [
    "`tidytext` also gives us a dataframe--`stop_words`--containing a list of _stopwords_ that we usually want to remove from a bag-of-words analysis.  Bag-of-words is really a _meaning_-centric kind of analysis, and stopwords (like _the, to, for, a, with in,_ etc) don't usually contribute a lot to the \"meaning\" of a document.  They do contribute a lot to the _structure_ (grammar/syntax) of a document, but in a bag-of-words analysis, we don't usually care about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a93d1d-149e-4a59-8e05-78a7c7c336dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 10 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>word</th><th scope=col>lexicon</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>a          </td><td>SMART</td></tr>\n",
       "\t<tr><td>a's        </td><td>SMART</td></tr>\n",
       "\t<tr><td>able       </td><td>SMART</td></tr>\n",
       "\t<tr><td>about      </td><td>SMART</td></tr>\n",
       "\t<tr><td>above      </td><td>SMART</td></tr>\n",
       "\t<tr><td>according  </td><td>SMART</td></tr>\n",
       "\t<tr><td>accordingly</td><td>SMART</td></tr>\n",
       "\t<tr><td>across     </td><td>SMART</td></tr>\n",
       "\t<tr><td>actually   </td><td>SMART</td></tr>\n",
       "\t<tr><td>after      </td><td>SMART</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 10 × 2\n",
       "\\begin{tabular}{ll}\n",
       " word & lexicon\\\\\n",
       " <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t a           & SMART\\\\\n",
       "\t a's         & SMART\\\\\n",
       "\t able        & SMART\\\\\n",
       "\t about       & SMART\\\\\n",
       "\t above       & SMART\\\\\n",
       "\t according   & SMART\\\\\n",
       "\t accordingly & SMART\\\\\n",
       "\t across      & SMART\\\\\n",
       "\t actually    & SMART\\\\\n",
       "\t after       & SMART\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 10 × 2\n",
       "\n",
       "| word &lt;chr&gt; | lexicon &lt;chr&gt; |\n",
       "|---|---|\n",
       "| a           | SMART |\n",
       "| a's         | SMART |\n",
       "| able        | SMART |\n",
       "| about       | SMART |\n",
       "| above       | SMART |\n",
       "| according   | SMART |\n",
       "| accordingly | SMART |\n",
       "| across      | SMART |\n",
       "| actually    | SMART |\n",
       "| after       | SMART |\n",
       "\n"
      ],
      "text/plain": [
       "   word        lexicon\n",
       "1  a           SMART  \n",
       "2  a's         SMART  \n",
       "3  able        SMART  \n",
       "4  about       SMART  \n",
       "5  above       SMART  \n",
       "6  according   SMART  \n",
       "7  accordingly SMART  \n",
       "8  across      SMART  \n",
       "9  actually    SMART  \n",
       "10 after       SMART  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(stop_words, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e8e4d-8134-4306-902f-bea5c0997adf",
   "metadata": {},
   "source": [
    "There are a few different stopword lists in this table.  Everyone and their cat has their own specialized stopword list, and really, most of them are pretty functionally indistinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a955fd68-b30d-4cc0-8497-d065b579f885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'SMART'</li><li>'snowball'</li><li>'onix'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'SMART'\n",
       "\\item 'snowball'\n",
       "\\item 'onix'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'SMART'\n",
       "2. 'snowball'\n",
       "3. 'onix'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"SMART\"    \"snowball\" \"onix\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique(stop_words$lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb493eb2-0e2f-4941-9376-b651a90417df",
   "metadata": {},
   "source": [
    "Let's be aggressive and remove any word that appears in any of these stopword lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e107ae-644d-4e25-9158-d30029d2a328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>review_id</th><th scope=col>stars</th><th scope=col>token</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>en_0964290</td><td>1</td><td>arrived     </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>en_0964290</td><td>1</td><td>broken      </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>en_0964290</td><td>1</td><td>manufacturer</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>en_0964290</td><td>1</td><td>defect      </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>en_0964290</td><td>1</td><td>legs        </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>en_0964290</td><td>1</td><td>base        </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>en_0964290</td><td>1</td><td>completely  </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>en_0964290</td><td>1</td><td>formed      </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>en_0964290</td><td>1</td><td>insert      </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>en_0964290</td><td>1</td><td>casters     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & review\\_id & stars & token\\\\\n",
       "  & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & en\\_0964290 & 1 & arrived     \\\\\n",
       "\t2 & en\\_0964290 & 1 & broken      \\\\\n",
       "\t3 & en\\_0964290 & 1 & manufacturer\\\\\n",
       "\t4 & en\\_0964290 & 1 & defect      \\\\\n",
       "\t5 & en\\_0964290 & 1 & legs        \\\\\n",
       "\t6 & en\\_0964290 & 1 & base        \\\\\n",
       "\t7 & en\\_0964290 & 1 & completely  \\\\\n",
       "\t8 & en\\_0964290 & 1 & formed      \\\\\n",
       "\t9 & en\\_0964290 & 1 & insert      \\\\\n",
       "\t10 & en\\_0964290 & 1 & casters     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 10 × 3\n",
       "\n",
       "| <!--/--> | review_id &lt;chr&gt; | stars &lt;int&gt; | token &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | en_0964290 | 1 | arrived      |\n",
       "| 2 | en_0964290 | 1 | broken       |\n",
       "| 3 | en_0964290 | 1 | manufacturer |\n",
       "| 4 | en_0964290 | 1 | defect       |\n",
       "| 5 | en_0964290 | 1 | legs         |\n",
       "| 6 | en_0964290 | 1 | base         |\n",
       "| 7 | en_0964290 | 1 | completely   |\n",
       "| 8 | en_0964290 | 1 | formed       |\n",
       "| 9 | en_0964290 | 1 | insert       |\n",
       "| 10 | en_0964290 | 1 | casters      |\n",
       "\n"
      ],
      "text/plain": [
       "   review_id  stars token       \n",
       "1  en_0964290 1     arrived     \n",
       "2  en_0964290 1     broken      \n",
       "3  en_0964290 1     manufacturer\n",
       "4  en_0964290 1     defect      \n",
       "5  en_0964290 1     legs        \n",
       "6  en_0964290 1     base        \n",
       "7  en_0964290 1     completely  \n",
       "8  en_0964290 1     formed      \n",
       "9  en_0964290 1     insert      \n",
       "10 en_0964290 1     casters     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized <- filter(tokenized, !(token %in% stop_words$word))\n",
    "head(tokenized, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0535af0-933a-442a-ae89-fefc91900fbd",
   "metadata": {},
   "source": [
    "Let's run the Porter Stemmer (a very imple stemming algorithm that is still empirically very useful--this is the same one that Gensim uses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1f5002-0c29-4a51-8dda-abbce2039dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>review_id</th><th scope=col>stars</th><th scope=col>token</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>en_0964290</td><td>1</td><td>arriv     </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>en_0964290</td><td>1</td><td>broken    </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>en_0964290</td><td>1</td><td>manufactur</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>en_0964290</td><td>1</td><td>defect    </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>en_0964290</td><td>1</td><td>leg       </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>en_0964290</td><td>1</td><td>base      </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>en_0964290</td><td>1</td><td>complet   </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>en_0964290</td><td>1</td><td>form      </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>en_0964290</td><td>1</td><td>insert    </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>en_0964290</td><td>1</td><td>caster    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & review\\_id & stars & token\\\\\n",
       "  & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & en\\_0964290 & 1 & arriv     \\\\\n",
       "\t2 & en\\_0964290 & 1 & broken    \\\\\n",
       "\t3 & en\\_0964290 & 1 & manufactur\\\\\n",
       "\t4 & en\\_0964290 & 1 & defect    \\\\\n",
       "\t5 & en\\_0964290 & 1 & leg       \\\\\n",
       "\t6 & en\\_0964290 & 1 & base      \\\\\n",
       "\t7 & en\\_0964290 & 1 & complet   \\\\\n",
       "\t8 & en\\_0964290 & 1 & form      \\\\\n",
       "\t9 & en\\_0964290 & 1 & insert    \\\\\n",
       "\t10 & en\\_0964290 & 1 & caster    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 10 × 3\n",
       "\n",
       "| <!--/--> | review_id &lt;chr&gt; | stars &lt;int&gt; | token &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | en_0964290 | 1 | arriv      |\n",
       "| 2 | en_0964290 | 1 | broken     |\n",
       "| 3 | en_0964290 | 1 | manufactur |\n",
       "| 4 | en_0964290 | 1 | defect     |\n",
       "| 5 | en_0964290 | 1 | leg        |\n",
       "| 6 | en_0964290 | 1 | base       |\n",
       "| 7 | en_0964290 | 1 | complet    |\n",
       "| 8 | en_0964290 | 1 | form       |\n",
       "| 9 | en_0964290 | 1 | insert     |\n",
       "| 10 | en_0964290 | 1 | caster     |\n",
       "\n"
      ],
      "text/plain": [
       "   review_id  stars token     \n",
       "1  en_0964290 1     arriv     \n",
       "2  en_0964290 1     broken    \n",
       "3  en_0964290 1     manufactur\n",
       "4  en_0964290 1     defect    \n",
       "5  en_0964290 1     leg       \n",
       "6  en_0964290 1     base      \n",
       "7  en_0964290 1     complet   \n",
       "8  en_0964290 1     form      \n",
       "9  en_0964290 1     insert    \n",
       "10 en_0964290 1     caster    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized$token <- wordStem(tokenized$token)\n",
    "head(tokenized, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989b0ed-d9d6-4e43-b5d4-bcd612609c8e",
   "metadata": {},
   "source": [
    "A few other pieces of cleanup: we'll remove all non-letter characters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378e6937-a7b6-43dc-97b4-9d20bc95baf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>review_id</th><th scope=col>stars</th><th scope=col>token</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>en_0964290</td><td>1</td><td>arriv     </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>en_0964290</td><td>1</td><td>broken    </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>en_0964290</td><td>1</td><td>manufactur</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>en_0964290</td><td>1</td><td>defect    </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>en_0964290</td><td>1</td><td>leg       </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>en_0964290</td><td>1</td><td>base      </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>en_0964290</td><td>1</td><td>complet   </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>en_0964290</td><td>1</td><td>form      </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>en_0964290</td><td>1</td><td>insert    </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>en_0964290</td><td>1</td><td>caster    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & review\\_id & stars & token\\\\\n",
       "  & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & en\\_0964290 & 1 & arriv     \\\\\n",
       "\t2 & en\\_0964290 & 1 & broken    \\\\\n",
       "\t3 & en\\_0964290 & 1 & manufactur\\\\\n",
       "\t4 & en\\_0964290 & 1 & defect    \\\\\n",
       "\t5 & en\\_0964290 & 1 & leg       \\\\\n",
       "\t6 & en\\_0964290 & 1 & base      \\\\\n",
       "\t7 & en\\_0964290 & 1 & complet   \\\\\n",
       "\t8 & en\\_0964290 & 1 & form      \\\\\n",
       "\t9 & en\\_0964290 & 1 & insert    \\\\\n",
       "\t10 & en\\_0964290 & 1 & caster    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 10 × 3\n",
       "\n",
       "| <!--/--> | review_id &lt;chr&gt; | stars &lt;int&gt; | token &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | en_0964290 | 1 | arriv      |\n",
       "| 2 | en_0964290 | 1 | broken     |\n",
       "| 3 | en_0964290 | 1 | manufactur |\n",
       "| 4 | en_0964290 | 1 | defect     |\n",
       "| 5 | en_0964290 | 1 | leg        |\n",
       "| 6 | en_0964290 | 1 | base       |\n",
       "| 7 | en_0964290 | 1 | complet    |\n",
       "| 8 | en_0964290 | 1 | form       |\n",
       "| 9 | en_0964290 | 1 | insert     |\n",
       "| 10 | en_0964290 | 1 | caster     |\n",
       "\n"
      ],
      "text/plain": [
       "   review_id  stars token     \n",
       "1  en_0964290 1     arriv     \n",
       "2  en_0964290 1     broken    \n",
       "3  en_0964290 1     manufactur\n",
       "4  en_0964290 1     defect    \n",
       "5  en_0964290 1     leg       \n",
       "6  en_0964290 1     base      \n",
       "7  en_0964290 1     complet   \n",
       "8  en_0964290 1     form      \n",
       "9  en_0964290 1     insert    \n",
       "10 en_0964290 1     caster    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove all non-letter characters.\n",
    "tokenized$token = gsub(\"[^a-z]+\", \"\", tokenized$token)\n",
    "head(tokenized, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ebb899-2cee-450e-97ee-1aff8fca6780",
   "metadata": {},
   "source": [
    "...and all words with only 1 or 2 characters (usually these are just noise)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aa2a62e-0ed8-4c97-9716-9539bdca5779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>review_id</th><th scope=col>stars</th><th scope=col>token</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>en_0964290</td><td>1</td><td>arriv     </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>en_0964290</td><td>1</td><td>broken    </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>en_0964290</td><td>1</td><td>manufactur</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>en_0964290</td><td>1</td><td>defect    </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>en_0964290</td><td>1</td><td>leg       </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>en_0964290</td><td>1</td><td>base      </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>en_0964290</td><td>1</td><td>complet   </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>en_0964290</td><td>1</td><td>form      </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>en_0964290</td><td>1</td><td>insert    </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>en_0964290</td><td>1</td><td>caster    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & review\\_id & stars & token\\\\\n",
       "  & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & en\\_0964290 & 1 & arriv     \\\\\n",
       "\t2 & en\\_0964290 & 1 & broken    \\\\\n",
       "\t3 & en\\_0964290 & 1 & manufactur\\\\\n",
       "\t4 & en\\_0964290 & 1 & defect    \\\\\n",
       "\t5 & en\\_0964290 & 1 & leg       \\\\\n",
       "\t6 & en\\_0964290 & 1 & base      \\\\\n",
       "\t7 & en\\_0964290 & 1 & complet   \\\\\n",
       "\t8 & en\\_0964290 & 1 & form      \\\\\n",
       "\t9 & en\\_0964290 & 1 & insert    \\\\\n",
       "\t10 & en\\_0964290 & 1 & caster    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 10 × 3\n",
       "\n",
       "| <!--/--> | review_id &lt;chr&gt; | stars &lt;int&gt; | token &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | en_0964290 | 1 | arriv      |\n",
       "| 2 | en_0964290 | 1 | broken     |\n",
       "| 3 | en_0964290 | 1 | manufactur |\n",
       "| 4 | en_0964290 | 1 | defect     |\n",
       "| 5 | en_0964290 | 1 | leg        |\n",
       "| 6 | en_0964290 | 1 | base       |\n",
       "| 7 | en_0964290 | 1 | complet    |\n",
       "| 8 | en_0964290 | 1 | form       |\n",
       "| 9 | en_0964290 | 1 | insert     |\n",
       "| 10 | en_0964290 | 1 | caster     |\n",
       "\n"
      ],
      "text/plain": [
       "   review_id  stars token     \n",
       "1  en_0964290 1     arriv     \n",
       "2  en_0964290 1     broken    \n",
       "3  en_0964290 1     manufactur\n",
       "4  en_0964290 1     defect    \n",
       "5  en_0964290 1     leg       \n",
       "6  en_0964290 1     base      \n",
       "7  en_0964290 1     complet   \n",
       "8  en_0964290 1     form      \n",
       "9  en_0964290 1     insert    \n",
       "10 en_0964290 1     caster    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove short tokens\n",
    "tokenized <- filter(tokenized, nchar(token) > 2)\n",
    "head(tokenized, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663e36d-5a63-40ef-b51e-574247863e7d",
   "metadata": {},
   "source": [
    "...and remove any words that appear in >50% of our documents (these are usually \"domain stopwords\", or words who tend to have a really flat distribution that our model can't learn much from), and in <10 documents (these are just rare words that don't really tell is much in the first place--and there will be a _lot_ of these).  I've picked these specific thresholds kind of arbitrarily; feel free to play around with them and see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5265d675-eec4-4cd1-8108-b4ec6317ce82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>review_id</th><th scope=col>stars</th><th scope=col>token</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>en_0537874</td><td>1</td><td>smudgi    </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>en_0947767</td><td>1</td><td>undeliver </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>en_0148393</td><td>1</td><td>electrocut</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>en_0655502</td><td>1</td><td>clanci    </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>en_0655502</td><td>1</td><td>uselessli </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>en_0243030</td><td>1</td><td>autoclav  </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>en_0243030</td><td>1</td><td>dermal    </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>en_0730545</td><td>1</td><td>havw      </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>en_0730832</td><td>1</td><td>outf      </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>en_0742404</td><td>1</td><td>peppercorn</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & review\\_id & stars & token\\\\\n",
       "  & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & en\\_0537874 & 1 & smudgi    \\\\\n",
       "\t2 & en\\_0947767 & 1 & undeliver \\\\\n",
       "\t3 & en\\_0148393 & 1 & electrocut\\\\\n",
       "\t4 & en\\_0655502 & 1 & clanci    \\\\\n",
       "\t5 & en\\_0655502 & 1 & uselessli \\\\\n",
       "\t6 & en\\_0243030 & 1 & autoclav  \\\\\n",
       "\t7 & en\\_0243030 & 1 & dermal    \\\\\n",
       "\t8 & en\\_0730545 & 1 & havw      \\\\\n",
       "\t9 & en\\_0730832 & 1 & outf      \\\\\n",
       "\t10 & en\\_0742404 & 1 & peppercorn\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 10 × 3\n",
       "\n",
       "| <!--/--> | review_id &lt;chr&gt; | stars &lt;int&gt; | token &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | en_0537874 | 1 | smudgi     |\n",
       "| 2 | en_0947767 | 1 | undeliver  |\n",
       "| 3 | en_0148393 | 1 | electrocut |\n",
       "| 4 | en_0655502 | 1 | clanci     |\n",
       "| 5 | en_0655502 | 1 | uselessli  |\n",
       "| 6 | en_0243030 | 1 | autoclav   |\n",
       "| 7 | en_0243030 | 1 | dermal     |\n",
       "| 8 | en_0730545 | 1 | havw       |\n",
       "| 9 | en_0730832 | 1 | outf       |\n",
       "| 10 | en_0742404 | 1 | peppercorn |\n",
       "\n"
      ],
      "text/plain": [
       "   review_id  stars token     \n",
       "1  en_0537874 1     smudgi    \n",
       "2  en_0947767 1     undeliver \n",
       "3  en_0148393 1     electrocut\n",
       "4  en_0655502 1     clanci    \n",
       "5  en_0655502 1     uselessli \n",
       "6  en_0243030 1     autoclav  \n",
       "7  en_0243030 1     dermal    \n",
       "8  en_0730545 1     havw      \n",
       "9  en_0730832 1     outf      \n",
       "10 en_0742404 1     peppercorn"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove terms that appear in >50% of all documents or <10 documents.\n",
    "common_terms <- (\n",
    "    tokenized\n",
    "    # convert to one row per document+token combo; i.e. deduplicate\n",
    "    # token entries within documents.\n",
    "    %>% unique() \n",
    "    %>% group_by(token)\n",
    "    %>% tally()\n",
    "    %>% mutate(pct = n / sum(n))\n",
    "    %>% filter(n > 10 & pct < 0.5)\n",
    ")\n",
    "tokenized <- filter(tokenized, !(token %in% common_terms$token))\n",
    "head(tokenized, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6e1b9-1868-42a9-b8a1-7bba8c0176ce",
   "metadata": {},
   "source": [
    "Note that since we're going to use a Bernoulli Naive Bayes--which assumed our features are only 0 or 1--we haven't done any steps to calculate the number of times each token appears in each document.  We could do this with a pretty simple line of code:\n",
    "\n",
    "```r\n",
    "tokenized <- (\n",
    "    tokenized\n",
    "    %>% group_by(review_id, token)\n",
    "    %>% tally()\n",
    ")\n",
    "```\n",
    "\n",
    "...but this is actually very slow and inefficient given the size of our data.  There's a lot we could do to make it go faster, probably, but we don't actually need to do this in the first place--remember that the Bernoulli Naive Bayes model we're going to use assumes all features are either 1 or 0.  And, if we just use the `cast_sparse` function from `tidytext`--which converts this \"long\" format into a `matrix`--we'll get a 1/0 matrix our anyways. `cast_sparse` doesn't do any sort of pivoting logic; it directly sets values in the output array, and if you have duplicates, they just overwrite each other.\n",
    "\n",
    "Long story short, `cast_sparse` will just give us the matris of only 1/0 unless we do the extra steps to calculate the counts beforehand.  So we just won't do that extra work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f12f64eb-408f-4ef6-a615-48891520a101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n",
      "[1] 0\n"
     ]
    }
   ],
   "source": [
    "# convert to a sparse Matrix so we can do some modeling.\n",
    "# There are also other cast_* functions to cast to different\n",
    "# formats, eg DocumentTermMatrix formats from the `tm` library\n",
    "# for topic modeling) or the DFM format used by Quanteda.\n",
    "train_bow <- cast_sparse(\n",
    "    tokenized[,c(\"review_id\", \"token\")],\n",
    "    review_id,\n",
    "    token\n",
    ")\n",
    "\n",
    "# verify that we only have 1s and 0s\n",
    "print(max(train_bow))\n",
    "print(min(train_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7aa63b-8f6c-49fa-a1a9-9526619eff23",
   "metadata": {},
   "source": [
    "We need to do a little extra bookkeeping to get the y values in the same order as the features after all this conversion.  None of the above steps will have changed the ordering of the documents, so we can safely get away with just a simple filter statement like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e14639-75d1-432f-b163-17bd45830736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] TRUE\n"
     ]
    }
   ],
   "source": [
    "# filter to just the same list of review ids\n",
    "train_y <- filter(train, review_id %in% rownames(train_bow))\n",
    "\n",
    "# this should just print \"TRUE\", meaning there's a one-to-one correspondence\n",
    "# between the review ids and the row labels in the bag of words matrix.\n",
    "print(unique(train_y$review_id == rownames(train_bow)))\n",
    "\n",
    "# pull out just the stars for use as our y-values for the naive bayes model\n",
    "train_y <- train_y$stars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2b010-eefa-4136-8b18-6d03445943b4",
   "metadata": {},
   "source": [
    "The above transformations are all _stateless;_ the only stateful part is the token filtering.  This is a bit of a problem for our data, since we have two different subsets (train and test), and we want to apply _the same transformations_ to the testing set as we do to the training set.  The `cast_sparse` step is the root of the issues here; we need some way to ensure that the columns are the same for the training and testing dataset.  But the identification of rare/common words to filter out is also a slight issue, albeit a much smaller one.\n",
    "\n",
    "So instead, we'll do a bit of an end run around the issue.  We'll concatenate our training and testing datasets together, run the transformations, and then split them back out into separate matrices based on the row names in the resulting matrix.  It's not as hacky as it sounds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a79864-394d-4450-a08d-1e577aabebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string2bow <- function (df) {\n",
    "    tokenized <- (\n",
    "        df[, c(\"Split\", \"review_body\", \"review_id\")]\n",
    "        # tokenize\n",
    "        %>% unnest_tokens(token, review_body)\n",
    "        # remove stopwords\n",
    "        %>% filter(!(token %in% stop_words$word))\n",
    "        # stem\n",
    "        %>% mutate(token = wordStem(token))\n",
    "        # remove non-alpha characters\n",
    "        %>% mutate(token = gsub(\"[^a-z]\", \"\", token))\n",
    "        # remove empty tokens and tokens <2 characters\n",
    "        %>% filter(nchar(token) > 2)\n",
    "    )\n",
    "    \n",
    "    # remove rare + common terms; but base this determination\n",
    "    # only on the training dataset.\n",
    "    common_terms <- (\n",
    "        tokenized\n",
    "        %>% filter(Split == \"Train\")\n",
    "        %>% group_by(token)\n",
    "        %>% tally()\n",
    "        %>% mutate(pct = n / sum(n))\n",
    "        %>% filter(!(n < 10 | pct > 0.5))\n",
    "    )\n",
    "    tokenized <- filter(tokenized, token %in% common_terms$token)\n",
    "    \n",
    "    # we need a numeric value to populate the sparse matrix with;\n",
    "    # this should be the word counts.\n",
    "    tokenized$n = 1\n",
    "    \n",
    "    return(tokenized)\n",
    "}\n",
    "\n",
    "# add indicator columns so we can split the datasets apart again later.\n",
    "train$Split <- \"Train\"\n",
    "test$Split <- \"Test\"\n",
    "data <- rbind(train, test)\n",
    "tokens <- string2bow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aac0d6-78ab-43ab-8e27-06cb58aaaec5",
   "metadata": {},
   "source": [
    "Now we cast to a sparse matrix and split the data back up into training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7bb97b-436f-495d-b444-13b96582c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow <- cast_sparse(tokens, review_id, token, n)\n",
    "\n",
    "# extract the y values and the split labels\n",
    "labels <- filter(data, review_id %in% rownames(bow))$stars\n",
    "splits <- filter(data, review_id %in% rownames(bow))$Split\n",
    "\n",
    "# break the data back out into train and test\n",
    "train_bow <- bow[splits == \"Train\",]\n",
    "train_y <- labels[splits == \"Train\"]\n",
    "\n",
    "test_bow <- bow[splits == \"Test\",]\n",
    "test_y <- labels[splits == \"Test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12913ed-1f4f-4bb9-b400-7bed67f3764b",
   "metadata": {},
   "source": [
    "That's it!  Now we're ready to train our Naive Bayes model and see how it does.  Since we have 5 classes and they're evenly balanced (I haven't explicitly calculated/shown that in this notebook, but it's easy to verify for yourself), we actually need to beat an accuracy/F1 of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "525226a4-410e-4684-90bf-19988dced5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"bernoulli_naive_bayes(): there are 1570 empty cells leading to zero estimates. Consider Laplace smoothing.\"\n"
     ]
    }
   ],
   "source": [
    "nb <- bernoulli_naive_bayes(train_bow, as.factor(train_y))\n",
    "preds <- predict(nb, newdata = test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5df6e57-8fe8-4b48-adf5-954afde14ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Accuracy:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.42606415170466"
      ],
      "text/latex": [
       "0.42606415170466"
      ],
      "text/markdown": [
       "0.42606415170466"
      ],
      "text/plain": [
       "[1] 0.4260642"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "mean(preds == test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff9d36a5-7c9c-4b77-9077-bfe892ff6781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"F1 score:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>.metric</th><th scope=col>.estimator</th><th scope=col>.estimate</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>f_meas</td><td>macro</td><td>0.411209</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 3\n",
       "\\begin{tabular}{lll}\n",
       " .metric & .estimator & .estimate\\\\\n",
       " <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t f\\_meas & macro & 0.411209\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 3\n",
       "\n",
       "| .metric &lt;chr&gt; | .estimator &lt;chr&gt; | .estimate &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| f_meas | macro | 0.411209 |\n",
       "\n"
      ],
      "text/plain": [
       "  .metric .estimator .estimate\n",
       "1 f_meas  macro      0.411209 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"F1 score:\")\n",
    "f_meas(\n",
    "    data = data.frame(preds = preds, true = as.factor(test_y)),\n",
    "    preds,\n",
    "    true,\n",
    "    beta = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442e2af-4aa1-439c-a3e3-ef94d3d6df9d",
   "metadata": {},
   "source": [
    "~0.4 isn't as high as it could be, but it's definitely better than 0.2!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
