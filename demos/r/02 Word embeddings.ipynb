{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba0eb11-ee43-46f2-bb0d-269d7705a33b",
   "metadata": {},
   "source": [
    "# Word Embeddings in R\n",
    "\n",
    "There are a handful of packages in R for working with word embeddings, but we'll use `text2vec` again.  We'll repeat the same prediction task as before, but rather than bag-of-words, we'll use word embeddings to represent the documents, and instead of a Naive Bayes model, we'll use a simple multi-layer perceptron.  (Word embeddings + highly nonlinear models, like tree-based models or neural networks, is a great combination).\n",
    "\n",
    "`text2vec` has tools to both load pre-trained word vectors and to train our own using the `GloVe` algorithm; we'll just train our own, since loading pre-trained ones requires going out and getting the vectors yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17af647-b63d-4388-bec4-792d16f6b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install if needed\n",
    "# install.packages(\"caret\")    # general machine learning library, provides a nice interface\n",
    "                               # to the RSNNS MLP implementation.\n",
    "# install.packages(\"dplyr\")    # general data munging\n",
    "# install.packages(\"RSNNS\")    # provides the MLP implementation we'll use\n",
    "# install.packages(\"text2vec\") # text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be503d49-4044-4a0d-887e-ae7d36bb209c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t200000 obs. of  8 variables:\n",
      " $ review_id       : chr  \"en_0964290\" \"en_0690095\" \"en_0311558\" \"en_0044972\" ...\n",
      " $ product_id      : chr  \"product_en_0740675\" \"product_en_0440378\" \"product_en_0399702\" \"product_en_0444063\" ...\n",
      " $ reviewer_id     : chr  \"reviewer_en_0342986\" \"reviewer_en_0133349\" \"reviewer_en_0152034\" \"reviewer_en_0656967\" ...\n",
      " $ stars           : int  1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ review_body     : chr  \"Arrived broken. Manufacturer defect. Two of the legs of the base were not completely formed, so there was no wa\"| __truncated__ \"the cabinet dot were all detached from backing... got me\" \"I received my first order of this product and it was broke so I ordered it again. The second one was broke in m\"| __truncated__ \"This product is a piece of shit. Do not buy. Doesn't work, and then I try to call for customer support, it won'\"| __truncated__ ...\n",
      " $ review_title    : chr  \"I'll spend twice the amount of time boxing up the whole useless thing and send it back with a 1-star review ...\" \"Not use able\" \"The product is junk.\" \"Fucking waste of money\" ...\n",
      " $ language        : chr  \"en\" \"en\" \"en\" \"en\" ...\n",
      " $ product_category: chr  \"furniture\" \"home_improvement\" \"home\" \"wireless\" ...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train <- read.csv(\"../../data/train.csv\", stringsAsFactors = FALSE)\n",
    "test <- read.csv(\"../../data/test.csv\", stringsAsFactors = FALSE)\n",
    "str(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8b8707-ef78-4d25-9572-f25f67f5cc51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "For binary classification, the first factor level is assumed to be the event.\n",
      "Use the argument `event_level = \"second\"` to alter this as needed.\n",
      "\n",
      "\n",
      "Attaching package: 'yardstick'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:caret':\n",
      "\n",
      "    precision, recall, sensitivity, specificity\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(dplyr)\n",
    "library(text2vec)\n",
    "library(yardstick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d979030-293d-4588-935b-31da679add00",
   "metadata": {},
   "source": [
    "The general workflow for training your own `GloVe` vectors is roughly as follows:\n",
    "- Tokenize your texts.\n",
    "- Filter our super rare tokens.\n",
    "- Build term-term co-occurrence matrix.\n",
    "- Use GloVe on that matrix to factorize it and get the word vectors.\n",
    "    - `text2vec` actually re-exports the GloVe implementation (and a few other things) from the `rsparse` package.  `rsparse` is designed for working with sparse matrices, including some common matrix factorization tools.  (GloVe is actually a matrix factorization algorithm!)\n",
    "\n",
    "\n",
    "Note that we aren't going to remove stopwords or super common words, and we aren't going to stem our texts.  Since word embeddings learn to encode _co-occurrence_ information, stopwords can actually provide useful information about how words are distributed.  E.g., a word that usually appears soon after _the_ is probably a noun or an adjective.  Different inflected forms of a word might also carry subtly different meanings that an embedding model can pick up on.  In theory, a bag-of-words model can pick up on this kind of meaning too, but doing so requires a massive increase in the sparsity and number of features, which can cause other reliability issues.  Since word embeddings are specifically designed to _not_ be sparse, we can do less preprocessing of our texts to preserve the maximum amount of information possible.\n",
    "\n",
    "We'll train our vectors based on just the training dataset.  Most of this is just about copy-pasted from the `text2vec` examples in the package's documentation, minus a bit of the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d81d3cd-7de2-4f33-ad1e-1cccced83475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A text2vec_vocabulary: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>term</th><th scope=col>term_count</th><th scope=col>doc_count</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>aaaand       </td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>aand         </td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>aas          </td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>aback        </td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>abandonment  </td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>abarth       </td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>abbreviated  </td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>abbreviations</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>abdl         </td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>abduction    </td><td>1</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A text2vec\\_vocabulary: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & term & term\\_count & doc\\_count\\\\\n",
       "  & <chr> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & aaaand        & 1 & 1\\\\\n",
       "\t2 & aand          & 1 & 1\\\\\n",
       "\t3 & aas           & 1 & 1\\\\\n",
       "\t4 & aback         & 1 & 1\\\\\n",
       "\t5 & abandonment   & 1 & 1\\\\\n",
       "\t6 & abarth        & 1 & 1\\\\\n",
       "\t7 & abbreviated   & 1 & 1\\\\\n",
       "\t8 & abbreviations & 1 & 1\\\\\n",
       "\t9 & abdl          & 1 & 1\\\\\n",
       "\t10 & abduction     & 1 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A text2vec_vocabulary: 10 × 3\n",
       "\n",
       "| <!--/--> | term &lt;chr&gt; | term_count &lt;int&gt; | doc_count &lt;int&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | aaaand        | 1 | 1 |\n",
       "| 2 | aand          | 1 | 1 |\n",
       "| 3 | aas           | 1 | 1 |\n",
       "| 4 | aback         | 1 | 1 |\n",
       "| 5 | abandonment   | 1 | 1 |\n",
       "| 6 | abarth        | 1 | 1 |\n",
       "| 7 | abbreviated   | 1 | 1 |\n",
       "| 8 | abbreviations | 1 | 1 |\n",
       "| 9 | abdl          | 1 | 1 |\n",
       "| 10 | abduction     | 1 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "   term          term_count doc_count\n",
       "1  aaaand        1          1        \n",
       "2  aand          1          1        \n",
       "3  aas           1          1        \n",
       "4  aback         1          1        \n",
       "5  abandonment   1          1        \n",
       "6  abarth        1          1        \n",
       "7  abbreviated   1          1        \n",
       "8  abbreviations 1          1        \n",
       "9  abdl          1          1        \n",
       "10 abduction     1          1        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create iterator over tokens.  The tokenization functions return\n",
    "# a list of list of tokens.\n",
    "tokens <- (\n",
    "    train$review_body\n",
    "    %>% tolower()\n",
    "    %>% gsub(\"[^a-z]+\", \" \", .)\n",
    "    %>% word_tokenizer()\n",
    "    %>% itoken()\n",
    ")\n",
    "vocab <- create_vocabulary(tokens)\n",
    "head(vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a5a375-6f05-4d55-bd7c-72dc39b476e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A text2vec_vocabulary: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>term</th><th scope=col>term_count</th><th scope=col>doc_count</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>abandon       </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>abnormal      </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>absorber      </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>abusing       </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>academic      </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>accentuate    </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>accentuated   </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>accentuates   </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>accomplishes  </td><td>5</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>accountability</td><td>5</td><td>5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A text2vec\\_vocabulary: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & term & term\\_count & doc\\_count\\\\\n",
       "  & <chr> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & abandon        & 5 & 5\\\\\n",
       "\t2 & abnormal       & 5 & 5\\\\\n",
       "\t3 & absorber       & 5 & 5\\\\\n",
       "\t4 & abusing        & 5 & 5\\\\\n",
       "\t5 & academic       & 5 & 5\\\\\n",
       "\t6 & accentuate     & 5 & 5\\\\\n",
       "\t7 & accentuated    & 5 & 5\\\\\n",
       "\t8 & accentuates    & 5 & 5\\\\\n",
       "\t9 & accomplishes   & 5 & 5\\\\\n",
       "\t10 & accountability & 5 & 5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A text2vec_vocabulary: 10 × 3\n",
       "\n",
       "| <!--/--> | term &lt;chr&gt; | term_count &lt;int&gt; | doc_count &lt;int&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | abandon        | 5 | 5 |\n",
       "| 2 | abnormal       | 5 | 5 |\n",
       "| 3 | absorber       | 5 | 5 |\n",
       "| 4 | abusing        | 5 | 5 |\n",
       "| 5 | academic       | 5 | 5 |\n",
       "| 6 | accentuate     | 5 | 5 |\n",
       "| 7 | accentuated    | 5 | 5 |\n",
       "| 8 | accentuates    | 5 | 5 |\n",
       "| 9 | accomplishes   | 5 | 5 |\n",
       "| 10 | accountability | 5 | 5 |\n",
       "\n"
      ],
      "text/plain": [
       "   term           term_count doc_count\n",
       "1  abandon        5          5        \n",
       "2  abnormal       5          5        \n",
       "3  absorber       5          5        \n",
       "4  abusing        5          5        \n",
       "5  academic       5          5        \n",
       "6  accentuate     5          5        \n",
       "7  accentuated    5          5        \n",
       "8  accentuates    5          5        \n",
       "9  accomplishes   5          5        \n",
       "10 accountability 5          5        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove rare tokens; the cutoff of 5 _tota counts_ (not _document counts_,\n",
    "# as with bag of words) is somewhat arbitrary on my part.\n",
    "vocab <- filter(vocab, term_count >= 5)\n",
    "head(vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b8dff9-2d11-486d-a73f-d477137ed4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [[ suppressing 33 column names 'abandon', 'abnormal', 'absorber' ... ]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10 x 17863 sparse Matrix of class \"dgTMatrix\"\n",
       "                                                                              \n",
       "abandon        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "abnormal       . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "absorber       . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "abusing        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "academic       . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "accentuate     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "accentuated    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "accentuates    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "accomplishes   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "accountability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
       "                       \n",
       "abandon        . ......\n",
       "abnormal       . ......\n",
       "absorber       . ......\n",
       "abusing        . ......\n",
       "academic       . ......\n",
       "accentuate     . ......\n",
       "accentuated    . ......\n",
       "accentuates    . ......\n",
       "accomplishes   . ......\n",
       "accountability . ......\n",
       "\n",
       " .....suppressing 17830 columns in show(); maybe adjust 'options(max.print= *, width = *)'\n",
       " .............................."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the vocabulary to create a vectorizer, then use\n",
    "# that vectorizer to create our term co-occurrence matrix\n",
    "# (\"tcm\" in text2vec lingo).\n",
    "vectorizer <- vocab_vectorizer(vocab)\n",
    "co_occurrence_matrix <- create_tcm(tokens, vectorizer, skip_grams_window = 5)\n",
    "head(co_occurrence_matrix, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d12b9-fd76-4532-ae26-884fcf2211e5",
   "metadata": {},
   "source": [
    "Now we fit the GloVe model.  We'll have it learn 300-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d659e0ff-c130-4a58-a663-a0b940f07403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  [11:01:14.215] epoch 1, loss 0.2387\n",
      "INFO  [11:01:20.213] epoch 2, loss 0.0909\n",
      "INFO  [11:01:26.215] epoch 3, loss 0.0658\n",
      "INFO  [11:01:32.237] epoch 4, loss 0.0485\n",
      "INFO  [11:01:38.333] epoch 5, loss 0.0416\n",
      "INFO  [11:01:44.948] epoch 6, loss 0.0368\n",
      "INFO  [11:01:51.552] epoch 7, loss 0.0336\n",
      "INFO  [11:01:58.826] epoch 8, loss 0.0310\n",
      "INFO  [11:02:05.513] epoch 9, loss 0.0289\n",
      "INFO  [11:02:12.040] epoch 10, loss 0.0271\n",
      "INFO  [11:02:18.571] epoch 11, loss 0.0257\n",
      "INFO  [11:02:25.151] epoch 12, loss 0.0244\n",
      "INFO  [11:02:31.735] epoch 13, loss 0.0232\n",
      "INFO  [11:02:38.273] epoch 14, loss 0.0222\n",
      "INFO  [11:02:44.786] epoch 15, loss 0.0213\n",
      "INFO  [11:02:51.082] epoch 16, loss 0.0205\n",
      "INFO  [11:02:57.506] epoch 17, loss 0.0197\n",
      "INFO  [11:03:03.888] epoch 18, loss 0.0191\n",
      "INFO  [11:03:10.509] epoch 19, loss 0.0184\n",
      "INFO  [11:03:17.163] epoch 20, loss 0.0179\n",
      "INFO  [11:03:23.351] epoch 21, loss 0.0174\n",
      "INFO  [11:03:30.527] epoch 22, loss 0.0169\n",
      "INFO  [11:03:38.083] epoch 23, loss 0.0164\n",
      "INFO  [11:03:47.193] epoch 24, loss 0.0160\n",
      "INFO  [11:03:55.102] epoch 25, loss 0.0156\n"
     ]
    }
   ],
   "source": [
    "# see the `rsparse` library's documentation for more details about\n",
    "# these functions.  This step might take a few minutes depending on\n",
    "# your hardware.\n",
    "glove = GlobalVectors$new(rank = 300, x_max=25)\n",
    "wv_main = glove$fit_transform(\n",
    "    co_occurrence_matrix,\n",
    "    n_iter = 25,\n",
    "    convergence_tol = 0.01,\n",
    "    # NOTE: SET THIS LOWER IF YOU HAVE FEWER THREADS AVAILABLE\n",
    "    # ON YOUR SYSTEM!\n",
    "    n_threads = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793f0e9-a6ce-4c9c-b806-a6150b8a67b5",
   "metadata": {},
   "source": [
    "Per the `text2vec` documentation, this learns two vectors for each word: a \"main\" and a \"context\" vector.  The package authors recommend averaging or summing these together; we'll just sum them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f6c8d0-79ef-40e9-afb5-b09908288fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_vectors = wv_main + t(glove$components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e720f1-8fe4-4a8e-858e-e5510e898fd3",
   "metadata": {},
   "source": [
    "Now we can get vectors for each word (truncated to just 10 dimensions, for the sake of sane output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be2fbdb-b4cf-42f5-8430-21e9c0f57520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>-0.327253146497863</li><li>0.260865638781066</li><li>-0.41142112850026</li><li>-0.053481625545049</li><li>0.300757341126326</li><li>-0.0592466607408711</li><li>0.574239726227137</li><li>-0.832201506629214</li><li>0.327204201289721</li><li>-0.743918991866967</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item -0.327253146497863\n",
       "\\item 0.260865638781066\n",
       "\\item -0.41142112850026\n",
       "\\item -0.053481625545049\n",
       "\\item 0.300757341126326\n",
       "\\item -0.0592466607408711\n",
       "\\item 0.574239726227137\n",
       "\\item -0.832201506629214\n",
       "\\item 0.327204201289721\n",
       "\\item -0.743918991866967\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. -0.327253146497863\n",
       "2. 0.260865638781066\n",
       "3. -0.41142112850026\n",
       "4. -0.053481625545049\n",
       "5. 0.300757341126326\n",
       "6. -0.0592466607408711\n",
       "7. 0.574239726227137\n",
       "8. -0.832201506629214\n",
       "9. 0.327204201289721\n",
       "10. -0.743918991866967\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] -0.32725315  0.26086564 -0.41142113 -0.05348163  0.30075734 -0.05924666\n",
       " [7]  0.57423973 -0.83220151  0.32720420 -0.74391899"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_vectors[\"abandon\", 1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ec64c-7e61-4cc9-b511-bad53b841f60",
   "metadata": {},
   "source": [
    "Our next step is going to be getting the summed word vectors for each document.  We're going to do this in a bit of a clever way: construct a document-term matrix (one row per document, one column per term, values are how many times that term appears in that document), then do a dot-product with the GloVe vectors.  For a very simple sketch of why this works, consider the two sentences _the cat sat on the mat_ and _the dog barked:_\n",
    "\n",
    "$$\n",
    "\\text{Vectorized} = \\textbf{DV}\n",
    "$$\n",
    "\n",
    "where $\\textbf{D}$ is our document-term matrix, and $\\textbf{V}$ is our array of word vectors.  Filling in some values:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{Vectorized} &= \\underbrace{\n",
    "        \\begin{bmatrix}\n",
    "            2 & 1 & 1 & 1 & 1 & 0 & 0 \\\\\n",
    "            1 & 0 & 0 & 0 & 0 & 1 & 1\n",
    "        \\end{bmatrix}\n",
    "    }_{\\textbf{D}}\n",
    "    \\underbrace{\n",
    "        \\begin{bmatrix}\n",
    "            v_{\\text{the}}\\\\\n",
    "            v_{\\text{cat}}\\\\\n",
    "            v_{\\text{sat}}\\\\\n",
    "            v_{\\text{on}}\\\\\n",
    "            v_{\\text{mat}} \\\\\n",
    "            v_{\\text{dog}} \\\\\n",
    "            v_{\\text{barked}}\n",
    "        \\end{bmatrix}\n",
    "    }_{\\textbf{V}}\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        2v_{\\text{the}} + v_{\\text{cat}} + v_{\\text{sat}} + v_{\\text{on}} + v_{\\text{mat}} \\\\\n",
    "        v_{\\text{the}} + v_{\\text{dog}} + v_{\\text{barked}}\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $v_i$ is the word embedding vector for word $i$.  Note that if we take $\\mathbf{D}$ and divide each row by its sum, so that each row sums to 1, this will calculate the average of the word vectors.\n",
    "\n",
    "We're doing it this way, rather than manually summing/averaging up vectors, mostly for efficiency/speed reasons.  This formulation lets us punt the computations off to very fast, efficient linear algebra libraries, while also writing way less code!  (realistically, explicitly summing the word vectors within each document probably isn't that much slower.  But this solution is so much cooler!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7606a462-f1b3-4502-9351-1ccf25b2c4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "as(<dgTMatrix>, \"dgCMatrix\") is deprecated since Matrix 1.5-0; do as(., \"CsparseMatrix\") instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtm <- create_dtm(tokens, vectorizer) %*% word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1079dbe1-66a5-467d-b702-ca191c362d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10 x 10 Matrix of class \"dgeMatrix\"\n",
       "         [,1]       [,2]       [,3]        [,4]      [,5]       [,6]       [,7]\n",
       "1  16.8847337 -1.5966930 12.3255083  8.08903599 8.8072155 26.6074137 -18.572676\n",
       "2   1.1124898 -0.1380209 -0.1977723 -0.60194675 0.7470795  1.9563452  -1.947656\n",
       "3   8.9582622 -1.2017779  4.9813878  7.33242057 3.8924063 10.9962288 -11.436741\n",
       "4   4.8123758 -1.5032346  2.8292792  4.93624175 3.0442225  8.2708395  -7.139036\n",
       "5   0.8562205  1.1252240  1.2295044  3.11522275 1.5732660  6.2266281  -2.640486\n",
       "6   4.7113183 -0.8208662  3.7866221  1.58915025 3.9922056  5.5243687  -2.180940\n",
       "7   2.8899061 -0.4078928  2.1513048  0.71102101 0.2343536  3.5852977  -3.728535\n",
       "8   2.3244985  0.1738621  1.1708788  1.03232248 1.3039819  0.9467438  -2.594803\n",
       "9   2.9768242 -1.0697795  0.9845830  0.07709319 4.5567791  5.8693569  -3.592678\n",
       "10 10.6938689 -0.6760623  7.0024062  6.36415726 8.0821253 16.1952134 -10.937990\n",
       "         [,8]      [,9]     [,10]\n",
       "1  -2.1972183 15.780257 24.285753\n",
       "2   0.5850388  1.229521  1.248588\n",
       "3   1.5504548 13.007919 10.440504\n",
       "4  -3.5520358  5.878076  7.443999\n",
       "5   0.4029675  3.010242  4.843457\n",
       "6   2.1483443  8.638991  6.023820\n",
       "7   0.2865646  4.087553  1.589680\n",
       "8   0.1087898  1.814156  1.112942\n",
       "9  -2.9917289  2.437958  7.845932\n",
       "10 -2.4990568 12.026424  9.244952"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dtm[1:10, 1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf207c-0ffb-4853-8f74-870f4d79273b",
   "metadata": {},
   "source": [
    "Let's wrap all this up in a single function to preprocess some texts once we've got our GloVe vectors trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "517f95d4-b8c0-4b2a-9e92-e56fb8a198d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess <- function(df, vectorizer) {\n",
    "    return (\n",
    "        df$review_body\n",
    "        %>% tolower()\n",
    "        %>% gsub(\"[^a-z]+\", \" \", .)\n",
    "        %>% word_tokenizer()\n",
    "        %>% itoken(progressbar = FALSE)\n",
    "        %>% create_dtm(vectorizer)\n",
    "    )\n",
    "}\n",
    "\n",
    "train_vecs = preprocess(train, vectorizer) %*% word_vectors\n",
    "train_y = train$stars\n",
    "test_vecs = preprocess(test, vectorizer) %*% word_vectors\n",
    "test_y = test$stars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a0751-7b58-42a8-b72b-0b15e1f8c1de",
   "metadata": {},
   "source": [
    "And now we train a simple multi-layer perceptron and evaluate its performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b8755c-0c24-462a-8f59-0f953570a471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multi-Layer Perceptron \n",
       "\n",
       "5000 samples\n",
       " 300 predictor\n",
       "   5 classes: '1', '2', '3', '4', '5' \n",
       "\n",
       "Pre-processing: centered (300), scaled (300) \n",
       "Resampling: Bootstrapped (25 reps) \n",
       "Summary of sample sizes: 5000, 5000, 5000, 5000, 5000, 5000, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  size  Accuracy   Kappa    \n",
       "  1     0.3496974  0.1886772\n",
       "  3     0.4231999  0.2790802\n",
       "  5     0.4277810  0.2847745\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# column names are required for training with caret; just use dummy ones\n",
    "# since the columns/features in the matrix don't have any meaningful direct\n",
    "# interpretation anyways.\n",
    "colnames(train_vecs) <- c(1:dim(train_vecs)[2])\n",
    "colnames(test_vecs) <- c(1:dim(test_vecs)[2])\n",
    "\n",
    "mlp <- train(\n",
    "    # training an MLP like this requires a Matrix object\n",
    "    # in order to do any automated preprocessing\n",
    "    x = as.matrix(test_vecs),\n",
    "    y = as.factor(test_y),\n",
    "    preProcess = c(\"center\", \"scale\"),\n",
    "    method =\"mlp\",\n",
    "    size = c(128, 64, 64),\n",
    "    maxit = 10\n",
    ")\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "005c4d68-2739-4f5b-acb4-fab593e26f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1</li><li>5</li><li>1</li><li>1</li><li>2</li><li>1</li></ol>\n",
       "\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<style>\n",
       "\t.list-inline {list-style: none; margin:0; padding: 0}\n",
       "\t.list-inline>li {display: inline-block}\n",
       "\t.list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "\t</style>\n",
       "\t<ol class=list-inline><li>'1'</li><li>'2'</li><li>'3'</li><li>'4'</li><li>'5'</li></ol>\n",
       "</details>"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item '1'\n",
       "\\item '2'\n",
       "\\item '3'\n",
       "\\item '4'\n",
       "\\item '5'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 5\n",
       "3. 1\n",
       "4. 1\n",
       "5. 2\n",
       "6. 1\n",
       "\n",
       "\n",
       "\n",
       "**Levels**: 1. '1'\n",
       "2. '2'\n",
       "3. '3'\n",
       "4. '4'\n",
       "5. '5'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 5 1 1 2 1\n",
       "Levels: 1 2 3 4 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds <- predict(mlp, as.matrix(test_vecs))\n",
    "head(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b42cead-1ecf-4dc2-9860-4ee907f8962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Accuracy:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.4854"
      ],
      "text/latex": [
       "0.4854"
      ],
      "text/markdown": [
       "0.4854"
      ],
      "text/plain": [
       "[1] 0.4854"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "mean(preds == test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab83cb7b-c08b-4ea3-a676-624895141d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"F1 score:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>.metric</th><th scope=col>.estimator</th><th scope=col>.estimate</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>f_meas</td><td>macro</td><td>0.4566993</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 3\n",
       "\\begin{tabular}{lll}\n",
       " .metric & .estimator & .estimate\\\\\n",
       " <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t f\\_meas & macro & 0.4566993\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 3\n",
       "\n",
       "| .metric &lt;chr&gt; | .estimator &lt;chr&gt; | .estimate &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| f_meas | macro | 0.4566993 |\n",
       "\n"
      ],
      "text/plain": [
       "  .metric .estimator .estimate\n",
       "1 f_meas  macro      0.4566993"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"F1 score:\")\n",
    "f_meas(\n",
    "    data = data.frame(preds = preds, true = as.factor(test_y)),\n",
    "    preds,\n",
    "    true,\n",
    "    beta = 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
