{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a09ba83-fbc9-46fe-85eb-33fcb6ea06b5",
   "metadata": {},
   "source": [
    "# Python demo: Bag-of-Words modeling with `gensim` and `scikit-learn`\n",
    "\n",
    "`scikit-learn`'s tools for bag-of-words models are great, but we usually need (or want) to exercise a bit more fine-grained control than we can get by just using the `CountVectorizer`/`TfidfVectorizer`/`HashingVectorizer`.  The `gensim` library gives us some nice tools to do this.  `gensim` isn't really designed to be a text preprocessing library--it's designed for things like training word vector models (see notebook 02), topic models, and similar things--but it has a set of very simple, extremely fast, surprisingly robust text cleaning and preprocessing tools that we can borrow.  We'll use these tools to clean up our documents and transform them into bag-of-words models, entirely within Gensim, then use the Bernoulli Naive Bayes classifier to build the predictive model.\n",
    "\n",
    "Pros:\n",
    "- Much more granular control over our preprocessing compared to `CountVectorizer`/etc, without adding a lot of complexity.\n",
    "- Still extremely fast--`gensim` is designed for speed and datasets of enormous size.\n",
    "\n",
    "Cons:\n",
    "- `gensim`'s preprocessing is really more _string processing_ than _language processing_.  This is part of why it's so fast, but it also means `gensim` knows nothing about, say, \"verbs\" or \"sentences.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42adeb1d-1406-4c1a-bf08-4e22d853702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "# !conda install --yes pandas tqdm gensim scikit-learn ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974c6104-2ac7-49db-9484-f36292bb941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is a magic library that gives you progerss bars when iterating\n",
    "# through things.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# register tqdm with pandas so we can get .progress_apply() method\n",
    "# added to dataframes.  This is a version of pd.DataFrame.apply()\n",
    "# but now it prints a progress bar!\n",
    "tqdm.pandas(smoothing=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04ee310-bd30-4ae3-91a6-71a6abcb7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "train = pd.read_csv(\"../../data/train.csv\")\n",
    "test = pd.read_csv(\"../../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acecf06-ade3-47fd-829f-cb14f58500e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: a helper function to absract the \"fit + predict + score\" logic.\n",
    "from sklearn import metrics\n",
    "\n",
    "def fit_and_score(clf, train_x, train_y, test_x, test_y):\n",
    "    \"\"\"fit the model `clf` to the `train` dataset and evaluate its\n",
    "    performance on the `test` dataset.\"\"\"\n",
    "    clf.fit(train_x, train_y)\n",
    "    preds = clf.predict(test_x)\n",
    "    \n",
    "    # calculate some classification metrics\n",
    "    accuracy = metrics.accuracy_score(preds, test_y)\n",
    "    f1 = metrics.f1_score(preds, test_y, average=\"macro\")\n",
    "\n",
    "    # and some regression metrics (since \"predict the number of stars\"\n",
    "    # could reasonably be either kind of task).\n",
    "    r2 = metrics.r2_score(preds, test_y)\n",
    "    mae = metrics.mean_absolute_error(preds, test_y)\n",
    "    \n",
    "    return pd.Series({\"Accuracy\": accuracy, \"F1\": f1, \"R2\": r2, \"MAE\": mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d3037-ca9b-48b6-9e3a-16ac9a4a5b06",
   "metadata": {},
   "source": [
    "Gensim's preprocessing tools are mostly in the `gensim.parsing.preprocessing` module.  There are a lot of smaller functions for specific tasks, but there's also `preprocess_string`, which applies a (very sensible) set of default preprocessing steps.  (We'll break this down into its component pieces in just a minute).  This function takes in a string, and returns a list of strings; i.e., it converts documents (strings) into lists of processed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c0fdba-fb7a-472a-9e7c-b606250a55e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2edcc011dd4f34a3b8677eac22d6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arriv', 'broken', 'manufactur', 'defect', 'leg', 'base', 'complet', 'form', 'wai', 'insert', 'caster', 'unpackag', 'entir', 'chair', 'hardwar', 'notic', 'spend', 'twice', 'time', 'box', 'useless', 'thing', 'send', 'star', 'review', 'chair', 'got', 'sit', 'far', 'includ', 'pictur', 'inject', 'mold', 'qualiti', 'assur', 'process', 'miss', 'hesit', 'bui', 'make', 'wonder', 'aren', 'miss', 'structur', 'support', 'imped', 'assembl', 'process']\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing import preprocessing as pre\n",
    "\n",
    "# default preprocessing pipeline.  lowercases, removes numbers/punctuation/\n",
    "# other funny characters, stems, and returns each document as a list of\n",
    "# string.\n",
    "preprocessed = train[\"review_body\"].progress_apply(pre.preprocess_string)\n",
    "print(preprocessed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91bb0b-fe16-4e60-92d7-835529a8c5ed",
   "metadata": {},
   "source": [
    "The list of preprocessing steps performed by `preprocess_string()` can be broken out using other functions from this same module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d108924-046b-466a-8490-6256b8c1b60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131e2adfb2da4da0a748febafa7ea5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arriv', 'broken', 'manufactur', 'defect', 'leg', 'base', 'complet', 'form', 'wai', 'insert', 'caster', 'unpackag', 'entir', 'chair', 'hardwar', 'notic', 'spend', 'twice', 'time', 'box', 'useless', 'thing', 'send', 'star', 'review', 'chair', 'got', 'sit', 'far', 'includ', 'pictur', 'inject', 'mold', 'qualiti', 'assur', 'process', 'miss', 'hesit', 'bui', 'make', 'wonder', 'aren', 'miss', 'structur', 'support', 'imped', 'assembl', 'process']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    \"\"\"preprocess a document and return a list of processd tokens.\"\"\"\n",
    "    # convert the text to lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # remove HTML/XML tags, which can show up a lot in data pulled from\n",
    "    # the internet.\n",
    "    s = pre.strip_tags(s)\n",
    "\n",
    "    # remove punctuation--rarely useful/needed for the things Gensim is designed\n",
    "    # to do.\n",
    "    s = pre.strip_punctuation(s)\n",
    "\n",
    "    # Replace multiple whitespaces with a single space\n",
    "    s = pre.strip_multiple_whitespaces(s)\n",
    "    \n",
    "    # remove numbers\n",
    "    s = pre.strip_numeric(s)\n",
    "    \n",
    "    # remove stopword\n",
    "    s = pre.remove_stopwords(s)\n",
    "    \n",
    "    # remove any short tokens (2 letters or less)\n",
    "    s = pre.strip_short(s)\n",
    "    \n",
    "    # run the text through the Porter stemmer\n",
    "    s = pre.stem_text(s)\n",
    "    \n",
    "    # split the string at whitespaces to get the list of tokens\n",
    "    return s.split()\n",
    "    \n",
    "preprocessed = train[\"review_body\"].progress_apply(preprocess)\n",
    "print(preprocessed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082825cc-0739-478b-8a19-bf183e385135",
   "metadata": {},
   "source": [
    "Now, we have to create a document-term matrix from these lists of lists of tokens.  `gensim` wants us to go about it this way:\n",
    "- Create a `gensim.corpora.Dictionary` object, which will store tokens, their raw frequencies, and their document frequencies (i.e. how many documents they appear in).\n",
    "- (optiona, but recommended) remove super rare and super frequent words from the `Dictionary`'s vocabulary list.\n",
    "- Use the `Dictionary` to transform our list of lists of tokens into a (`gensim`-specific) sparse matrix format.\n",
    "- Use `gensim.matutils` module to convert into a Scipy sparse matrix format, so we can use it with `scikit-learn` models.\n",
    "\n",
    "It's not as much code as it sounds.\n",
    "\n",
    "_Note:_ we very easily could just stop here and go straight to `scikit-learn`.  All we'd need to to is skip the `s.split()` line in the `preprocess()` function we just wrote (so we get back strings, not lists of strings), but then we could pass our `preprocessed` object directly in as our `X` value to a `CountVectorizer` + `BernoulliNB` pipeline.  This is probably a good idea, since the next few cells are basically just replicating the workof the `CountVectorizer` transformer, but these steps are required for using almost any of `gensim`'s own models (which we'll do in notebook 02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bebba5a-bb85-4531-bd5d-e9295a0ef7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (27, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# gensim.corpora.Dictionary objects find all of our unique vocabulary,\n",
    "# can filter out super rare/common terms, and can then efficiently transform\n",
    "# processed dodcument into bag-of-words formats.\n",
    "id2word = Dictionary(preprocessed)\n",
    "\n",
    "# Remove very rare and very common words--this operation happens in-place.\n",
    "id2word.filter_extremes(\n",
    "    # pass a float between 0 and 1 --> remove any token in more/fewer than\n",
    "    # that *percent* of documents.  Pass an integer --> remove any token\n",
    "    # that appears in more/fewer than exactly that many documents.\n",
    "    no_above=0.5,\n",
    "    no_below=10,\n",
    ")\n",
    "\n",
    "# Dictionary.doc2bow(list_of_tokens) will convert list_of_tokens into\n",
    "# a Gensim-internal sparse matrix format.\n",
    "bow = [id2word.doc2bow(i) for i in preprocessed]\n",
    "print(bow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb82f7-15b0-495d-a7f8-19bb191e6627",
   "metadata": {},
   "source": [
    "To convert this format (a list of length-2 tuples) into a `scikit-learn` compatible format, we need to grab `gensim.matutils.corpus2csc`--which will convert this into a _compressed sparse column (CSC)_ matrix--and then transpose it via the `.T` attribute.  `corpus2csc` gives us a sparse matrix with _one column per observation, one row per feature_, but `scikit-learn` expects _one row per observation, one column per feature_, hence the transpose operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f39d9b9-6e61-43aa-aa06-6a38c8c73301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2csc\n",
    "\n",
    "# csc = compressed sparse column = one column per document.\n",
    "# Need to transpose it to get one document per row, which is what\n",
    "# scikit-learn models expect.  This also convert it to a compressed\n",
    "# sparse row format for us.\n",
    "bow_train = corpus2csc(bow).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9712a5-5c1e-49ae-80d2-62ea03d0a284",
   "metadata": {},
   "source": [
    "We can combine these steps (minus training the `Dictionary` object) together into a series of chained `pandas` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efeafb0-c6ea-44f8-9809-822db86a0b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a97d2dd12a1436b9aad07426a785c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acbe787ab434edda3c8fc94bed2ce1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bow_test = (\n",
    "    test[\"review_body\"]\n",
    "    .progress_apply(preprocess)\n",
    "    .progress_apply(id2word.doc2bow)\n",
    "    # need to specify num_terms for corpus2csc, otherwise\n",
    "    # we might end up with all-zero columns being dropped.\n",
    "    # that will cause issues later for the naive bayes model.\n",
    "    .pipe(corpus2csc, num_terms=len(id2word))\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd4491f-9af1-411e-8ce3-3dda1483b020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy    0.450800\n",
       "F1          0.430264\n",
       "R2          0.304645\n",
       "MAE         0.865000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "fit_and_score(\n",
    "    BernoulliNB(),\n",
    "    bow_train,\n",
    "    train[\"stars\"],\n",
    "    bow_test,\n",
    "    test[\"stars\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de23cb-ce8c-4bea-998f-b08f39bf50f4",
   "metadata": {},
   "source": [
    "`gensim` really shines in other areas than its text preprocessing; its preprocessing tools, while convenient and fast, aren't really doing anything fancy.  Under the hood, most of it is just regular expressions and removing word that appear in a pre-made stopword list.  You could probably implement these functions yourself pretty easily (and it's a good exercize to try doing that), but `gensim` provides them in one nice, convenient place.  And, compared to more linguistically-savvy methods (like `spacy`, which we'll see in the notebok 01c), `gensim` is extremely fast.  If you find yourself with a stupidly large corpus, `gensim` might be a good preprocessing join just because of its speed.\n",
    "\n",
    "In notebook 02 we'll see some things `gensim` is much more specialized in and much better at, namely training our own Word2Vec models.  `gensim`'s real killer features are that kind of model--the (mostly unsupervised) text models like embeddings and topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103395d3-7b5b-4ead-892e-8a23f911f156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
