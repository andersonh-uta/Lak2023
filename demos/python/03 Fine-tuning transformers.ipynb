{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2f7af9-3c80-4fea-898e-a2c3567284e7",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT-style models\n",
    "\n",
    "Transformer-based large language models have been all the hype since about 2018 when BERT was first published.  Fortunately, it's not too hard to fine-tune a model (or, at least, do do some quick-and-dirty fine tuning; optimizing the fine-tuning process can be a pretty time-intensive process).  the `transformers` library from Huggingface is the single best tool for working with transformer-based language models.  Couple it with PyTorch (or, increasingly, Jax+Flax) and you've got a pretty easy to use toolbox--as long as you have the GPU compute.  (You do NOT want to try to run this on a CPU-only machine; it'll just be too slow).\n",
    "\n",
    "We'll use PyTorch to fine-tune an DistilBERT model on the Amazon review data, and we'll mostly follow the recommended settings from Huggingface. DistilBERT is a \"distilled\" version of BERT that's about half the size, about twice the speed, and about 97% the accuracy.  This code successfully ran on an NVidia Quadro T1000 card (4GB VRAM) that was not being used for anything else, e.g. not being used for graphics rendering.  You _could_ run this code purely on CPU and worry less about the memory overhead, but it'll be a lot slower.\n",
    "\n",
    "## A note about pretraining and fine-tuning\n",
    "\n",
    "One of the big reasons why transformers have become all the rage is the \"pretrain-finetune\" paradigm, which is essentially a form of transfer learning.  The model is first _pre-trained_ on a self-supervised language task, usually a _masked language task_.  Some strategy is used to hide tokens in the input texts, and the model has to predict what word has been hidden.  Then, to fine-tune the model, you add a small densely connected layer right at the very end, feed example sentences through, and only update the dense layer and the last few of the pretrained models.\n",
    "\n",
    "Conceptually, there are two ways to think about this:\n",
    "1. The pre-training step lets the model learn some general representation of the target language (e.g. English).  I.e., it imbues the model with some information that answers the question: \"what does English generally look like?\"  The, the fine-tuning step takes this general representation of a language, and hones it to be really good at one specific task.  Once a model knows what English generally looks like, it can then learn a more specialized representation.\n",
    "\n",
    "1. The pre-training step is a way to learn a really good, really general-purpose initialization of the neural network, which serves as a good initialization for a wide range of downstream tasks.  Compare this to, e.g., a random initialization of the network weights.  Fine-tuning is taking this initialization and building the \"real\" model on top of it.\n",
    "\n",
    "Pre-training is an extraordinarily time- and compute-intensive process, so you'll probably never do that yourself.  Pretraining models can take weeks or months of continuous runtime on fairly large servers/clusters.  Fine-tuning, by contrast, is relatively quick (though it can still be slow, since these are still quite large neural network models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986587c6-4f6c-42f0-b86d-6540eb97021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "# !conda install --yes tqdm pandas scikit-learn\n",
    "\n",
    "# NOTE: go to https://pytorch.org/get-started/locally/ and replace the next line\n",
    "# with the installation instructions for your platform.\n",
    "# !conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "# !conda intall --yes -c huggingface huggingface\n",
    "# !python -m pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e5175d-547f-4699-a1a1-879593230447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is a magic library that gives you progerss bars when iterating\n",
    "# through things.\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5d8060-efcc-41a7-b7fd-306668f0456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "train = pd.read_csv(\"../../data/train.csv\")\n",
    "test = pd.read_csv(\"../../data/test.csv\")\n",
    "val = pd.read_csv(\"../../data/validation.csv\")\n",
    "\n",
    "# the Transformer models will expect our labels to be numeric\n",
    "# indices starting from 0; just subtract 1 from our stars and\n",
    "# we're good.  (and add 1 to the final predicted number of stars\n",
    "# to convert back).\n",
    "train[\"stars\"] -= 1\n",
    "test[\"stars\"] -= 1\n",
    "val[\"stars\"] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d260d22-7958-4255-a812-af19e40f768d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87646392-f542-44b8-9d9c-0d87a87876d9",
   "metadata": {},
   "source": [
    "In Huggingace, we generally need to manually run the model's tokenizer over our data.  Transformer-based models have learnable/trainable tokenizers that, essentially, learn how to tokenize an input text; this means that different models' tokenizers behave differently.  It also means that the tokenization is not necessarily human-understandable after it's done.\n",
    "\n",
    "To get both the full model (which we'll fine-tune in a moment) and the tokenizer (which we'll use to tokenize our data), we just use the `.from_pretrained()` method on the `Auto*` classes.  Here, we're loading the `albert-base-v2` model to start with; this is a transformer model that's designed to be much smaller and faster than BERT, but without giving up much accuracy.  (ALBERT has ~11M parameters, compared to BERT's ~110M).  We're using ALBERT purely for the speed and low memory footprint--feel free to swap it out for a larger model, like `bert-base-uncased`, if you've got a decent GPU and want to run this yourself.\n",
    "\n",
    "_Note:_ to use a different base model, like `bert-base-uncased`, `roberta-base`, etc, just replace the name of the model being loaded.  The rest of your code remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbb3d79-f46e-472d-8b90-79aba56e843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# the model, which we'll fine-tune for our classification task.\n",
    "# it will be downloaded+cached if not already available locally.\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    \n",
    "    # tell it how many labels we have--this will add the dense\n",
    "    # layer at the end of the model.\n",
    "    num_labels=5,\n",
    "    \n",
    "    # rather than the default 32-bit floating points, load the model\n",
    "    # in 16-bit float format if there isn't a GPU available.\n",
    "    # This will roughly halve the memory use, and will usually provide\n",
    "    # a speedup.  This will NOT provide a GPU speedup for all GPUs;\n",
    "    # some more recent NVidia GPUs have highly-optimized 16-bit float\n",
    "    # operations, and for those, this can provide about a 2x speed increase.\n",
    "    # But in the interest of compatibility--and accuracy--we'll leave it\n",
    "    # at float32 and just eat the speed cost.\n",
    "    #\n",
    "    # However: note that not all models are necessarily compatible with\n",
    "    # half-precision (16-bit float) on all hardware.  So if you want to\n",
    "    # play with half-precision floats for the extra speed, it'll involve\n",
    "    # some experimentation.\n",
    "    torch_dtype=torch.float32 if torch.cuda.is_available() else torch.float16,\n",
    ")\n",
    "\n",
    "# the tokenizer, which we'll use to manually preprocess our data.\n",
    "# it will be downloaded+cached if not already available locally.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e9682-ee35-46fb-a100-f37dc3c8d4da",
   "metadata": {},
   "source": [
    "We can ignore those warnings.\n",
    "\n",
    "The tokenizer and model are both callable objects.  Calling the tokenizer on some strings will tokenize them; calling the model on some tokenized texts will run the inputs through the model and generate final predictions.  But, we're not going to tokenize our dataset right now; doing that would use up a huge amount of RAM.  Instead, we'll make a custom PyTorch `Dataset` class that handles iteration through our raw strings, and we'll tokenize them on-the-fly during the training loop.  This sacrifices some speed, but it'll save us a lot of RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a7c4e8-01cb-4520-b839-e1e986c8e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "def tokenize(texts, tokenizer):\n",
    "    \"\"\"Tokenize the texts, so the model can take them as inputs.\"\"\"\n",
    "    return tokenizer(\n",
    "        # the texts must be a list--not an \"array-like,\" but an actual\n",
    "        # list--if we're passing multiple texts at once.\n",
    "        list(texts),\n",
    "\n",
    "        # pad short texts out to the max sequence length for the model.\n",
    "        padding=\"max_length\",\n",
    "\n",
    "        # truncate long texts to the model's max length.  Truncation is\n",
    "        # rarely an issue for texts that are just a bit longer than the\n",
    "        # model's max length, but it can introduce errors for texts that\n",
    "        # are very long.  (very long texts are a pathological problem for\n",
    "        # transformers).\n",
    "        truncation=True,\n",
    "\n",
    "        # return PyTorch tensors, since we're going to use PyTorch models\n",
    "        # and PyTorch training loops.\n",
    "        # (other options are \"tf\" for Tensorflow tensors, or \"np\" for\n",
    "        # Numpy ndarrays).\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "def make_dataset(df, batch_size=8):\n",
    "    return DataLoader(\n",
    "        TextDataset(df[\"review_body\"], df[\"stars\"]),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "train_dataset = make_dataset(train, batch_size=8)\n",
    "test_dataset = make_dataset(test, batch_size=32)\n",
    "val_dataset = make_dataset(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4383fc-527f-4d48-a9c2-03ab05e1f4b1",
   "metadata": {},
   "source": [
    "As a quick sidebar, the tokenizers return a Python `dict` that we'll pass to the model using `**kwargs` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6482f2ae-84a1-43d4-92e4-3bafd3586813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "tensor([[ 101, 2023, 2003, 1037, 6251, 1012,  102,    0,    0,    0]])\n",
      "\n",
      "attention_mask\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (k,v) in tokenize([\"This is a sentence.\"], tokenizer).items():\n",
    "    print(k)\n",
    "    print(v[:, :10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6724e0-35a5-4551-8178-73ab895665f1",
   "metadata": {},
   "source": [
    "Now, we can start up our PyTorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1735a4-6d9b-43ef-9270-b8d8bd85d66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbcb294caf14adcb0c740279084910f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250 - validation loss=1.1785367727279663\n",
      "Batch 500 - validation loss=1.12093186378479\n",
      "Batch 750 - validation loss=1.0768646001815796\n",
      "Batch 1,000 - validation loss=1.0647107362747192\n",
      "Batch 1,250 - validation loss=1.0456185340881348\n",
      "Batch 1,500 - validation loss=1.0505222082138062\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# AdamW is a standard optimizer for transformers.\n",
    "# Sometimes you'll see regular Adam or something else,\n",
    "# but usually the optimizers aren't anything super\n",
    "# exotic for transformer models.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Learning rate scheduler--also commonly used for fine-tuning transformers.\n",
    "# This will linearly decrease the learning rate after each training batch.\n",
    "# For this demo we'll only train for a single epoch over the training data.\n",
    "n_epochs = 1\n",
    "n_train_steps = n_epochs * len(train_dataset)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=n_train_steps\n",
    ")\n",
    "\n",
    "# use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# training loop!\n",
    "# yes, the indents move about half a mile to the right, and I'm sorry\n",
    "# about that.\n",
    "model.to(device)\n",
    "batchnum = 0\n",
    "best_val_loss = 100\n",
    "early_stopping_patience = 0\n",
    "val_losses = []\n",
    "val_batches = []\n",
    "train_losses = []\n",
    "for (x, y) in tqdm(train_dataset, desc=f\"Training\"):\n",
    "\n",
    "    # tokenize the strings and move them to the GPU, if a GPU is available.\n",
    "    batch = {\n",
    "        k: v.to(device)\n",
    "        for (k,v) in tokenize(x, tokenizer).items()\n",
    "    }\n",
    "    batch[\"labels\"] = y.to(device)\n",
    "\n",
    "    # transformers track their own loss; when the `batch` dict has\n",
    "    # a `labels` field.  We could use our own loss calculation, but\n",
    "    # this is fine.\n",
    "    preds = model(**batch)\n",
    "    loss = preds.loss\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization and learning rate steps\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # reset optimizer learning rate\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batchnum += 1\n",
    "\n",
    "    # evaluate on the validation data every 100 batches\n",
    "    if batchnum % 250 == 0:\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (x, y) in val_dataset:\n",
    "                batch = {\n",
    "                    k: v.to(device)\n",
    "                    for (k,v) in tokenize(x, tokenizer).items()\n",
    "                }\n",
    "                batch[\"labels\"] = y.to(device)\n",
    "                preds = model(**batch)\n",
    "                loss = preds.loss\n",
    "                val_loss += loss\n",
    "        val_loss = val_loss / len(val_dataset)\n",
    "        print(f\"Batch {batchnum:,} - validation loss={val_loss}\")\n",
    "        val_losses.append(val_loss)\n",
    "        val_batches.append(batchnum)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        # simple early stopping criteria--stop as soon as the training loss\n",
    "        # doesn't decrease between validation rounds.  I've left the code\n",
    "        # here for doing a more patient approach to early stopping--just\n",
    "        # change the `1` in `if early_stopping_patience >= 1` to be however\n",
    "        # many validation rounds with no improvement you want to wait before \n",
    "        # stopping training.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_patience = 0\n",
    "        else:\n",
    "            early_stopping_patience += 1\n",
    "        \n",
    "        if early_stopping_patience >= 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35823f30-95f2-40b7-bdc6-5b25a696ed98",
   "metadata": {},
   "source": [
    "You can see from the `tqdm` timer bar how long this took--a bit under an hour on my computer.  Granted, I ran this on a fairly basic GPU (NVidia Quadro T1000--a pretty entry-level, general-purpose compute GPU for laptops), but even on higher end hardware this will still take some time.\n",
    "\n",
    "Just for reference, here are some other speed benchmarks from running this same code on a few different GPU configurations:\n",
    "- CPU-only (Intel Xeon E-2276M, 2.8GHz base clock): ~10-12s per batch.\n",
    "- CPU-only (AMD Threadripper 2990WX, 3.0GHz base clock): ~2.5s per batch.\n",
    "- NVidia Quadro T1000: ~1s per batch.\n",
    "- NVidia Titan V: ~6.5 batches per second.\n",
    "\n",
    "So, yes, using a good GPU is a _huge_ must, if you have one available, and if you have a lot of data.  And a GPU with more VRAM means you can run bigger models, too, which can often give you more accuracy (at the cost of longer training times).\n",
    "\n",
    "Anyways. Let's check the final accuracy/F1 scores of our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec980f6-f86f-42f2-8211-d18c28466994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855df8fb0850431687fc1ca796a168c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    predicted = []\n",
    "    ys = []\n",
    "    model.cuda()\n",
    "    for (x, y) in tqdm(test_dataset):\n",
    "        batch = {k:v.to(device) for k,v in tokenize(x, tokenizer).items()}\n",
    "        preds = model(**batch)\n",
    "        predicted.append(preds[\"logits\"].argmax(axis=1).detach().cpu().numpy())\n",
    "        ys.append(y.detach().numpy())\n",
    "\n",
    "predicted = np.hstack(predicted)\n",
    "ys = np.hstack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b90147-fc14-41b4-9ba9-e80900fc943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.98%\n",
      "Macro F1: 0.5271\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(predicted == ys)\n",
    "f1 = metrics.f1_score(predicted, ys, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy: {acc:.2%}\")\n",
    "print(f\"Macro F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb603f-14e0-4f1a-93d7-40ed8626da27",
   "metadata": {},
   "source": [
    "Notice how the code we wrote--minus the PyTorch training loop part--was about as complex as the other notebooks.  But also note that our total runtime and hardware requirements went up considerably, and our accuracy went up by a noticeable, but not overwhelming, margin.  This goes back to what I said in the first notebook: there will rarely be a situation where transformers have awesome performance, but simpler models have garbage performance.  (There are a few, but they're rare, and usually deal with extremely abstract, latent linguistic constructs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
