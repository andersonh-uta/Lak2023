{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a09ba83-fbc9-46fe-85eb-33fcb6ea06b5",
   "metadata": {},
   "source": [
    "# Python demo: Bag-of-Words modeling with `spacy` + `scikit-learn`\n",
    "\n",
    "`spacy` is a library that provides the next level of linguistic sophistication and control.  Unlike `gensim`, which is primarily built on _string processing_ operations, `spacy` is built to be much more linguistically-savvy.  It knows about things like verb, nouns, and syntax, and it's probably the single best go-to tool for general-purpose linguistic annotations and parsing.  But, this comes at a cost: it's slower than `gensim` (by quite a lot), and it uses a lot more memory (since it uses a number of machine learning models to do all of its annotations).\n",
    "\n",
    "We'll use `spacy` to recreate most of what we did with `gensim` in notebook 01b.\n",
    "\n",
    "Pros:\n",
    "- Much deeper, richer linguistic annotations: you can add features like part of speech, named entity tags, syntactic information, etc., and enrich your data.\n",
    "- Very fast given how much it's doing; still slower than `gensim` or a pure-`scikit-learn` solution, but much faster than many tools that apply similar kind of annotations.\n",
    "- Extremely easy to use; the API is super easy to get started with.\n",
    "- Models can be run on a GPU for extra speed.\n",
    "- You can train your own `spacy` models to do different annotation tasks.  How to do this goes well beyond the scope of this notebook, but it's very doable.\n",
    "\n",
    "Cons:\n",
    "- Slower--in absolute terms--than something like `gensim`.  (because it's just doing more stuff).\n",
    "- Requires a bit more coding to really make use of its features--it's a surprisingly deep library.\n",
    "- The API can be unexpectedly deep, which can lead to occasional \"footgun\" moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9258773-dabb-4858-aa3a-81b8cd94c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "# !conda install --yes tqdm pandas scikit-learn spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974c6104-2ac7-49db-9484-f36292bb941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is a magic library that gives you progerss bars when iterating\n",
    "# through things.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# register tqdm with pandas so we can get .progress_apply() method\n",
    "# added to dataframes.  This is a version of pd.DataFrame.apply()\n",
    "# but now it prints a progress bar!\n",
    "tqdm.pandas(smoothing=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04ee310-bd30-4ae3-91a6-71a6abcb7cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en_0964290</td>\n",
       "      <td>product_en_0740675</td>\n",
       "      <td>reviewer_en_0342986</td>\n",
       "      <td>1</td>\n",
       "      <td>Arrived broken. Manufacturer defect. Two of th...</td>\n",
       "      <td>I'll spend twice the amount of time boxing up ...</td>\n",
       "      <td>en</td>\n",
       "      <td>furniture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en_0690095</td>\n",
       "      <td>product_en_0440378</td>\n",
       "      <td>reviewer_en_0133349</td>\n",
       "      <td>1</td>\n",
       "      <td>the cabinet dot were all detached from backing...</td>\n",
       "      <td>Not use able</td>\n",
       "      <td>en</td>\n",
       "      <td>home_improvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en_0311558</td>\n",
       "      <td>product_en_0399702</td>\n",
       "      <td>reviewer_en_0152034</td>\n",
       "      <td>1</td>\n",
       "      <td>I received my first order of this product and ...</td>\n",
       "      <td>The product is junk.</td>\n",
       "      <td>en</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en_0044972</td>\n",
       "      <td>product_en_0444063</td>\n",
       "      <td>reviewer_en_0656967</td>\n",
       "      <td>1</td>\n",
       "      <td>This product is a piece of shit. Do not buy. D...</td>\n",
       "      <td>Fucking waste of money</td>\n",
       "      <td>en</td>\n",
       "      <td>wireless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en_0784379</td>\n",
       "      <td>product_en_0139353</td>\n",
       "      <td>reviewer_en_0757638</td>\n",
       "      <td>1</td>\n",
       "      <td>went through 3 in one day doesn't fit correct ...</td>\n",
       "      <td>bubble</td>\n",
       "      <td>en</td>\n",
       "      <td>pc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_id          product_id          reviewer_id  stars  \\\n",
       "0  en_0964290  product_en_0740675  reviewer_en_0342986      1   \n",
       "1  en_0690095  product_en_0440378  reviewer_en_0133349      1   \n",
       "2  en_0311558  product_en_0399702  reviewer_en_0152034      1   \n",
       "3  en_0044972  product_en_0444063  reviewer_en_0656967      1   \n",
       "4  en_0784379  product_en_0139353  reviewer_en_0757638      1   \n",
       "\n",
       "                                         review_body  \\\n",
       "0  Arrived broken. Manufacturer defect. Two of th...   \n",
       "1  the cabinet dot were all detached from backing...   \n",
       "2  I received my first order of this product and ...   \n",
       "3  This product is a piece of shit. Do not buy. D...   \n",
       "4  went through 3 in one day doesn't fit correct ...   \n",
       "\n",
       "                                        review_title language  \\\n",
       "0  I'll spend twice the amount of time boxing up ...       en   \n",
       "1                                       Not use able       en   \n",
       "2                               The product is junk.       en   \n",
       "3                             Fucking waste of money       en   \n",
       "4                                             bubble       en   \n",
       "\n",
       "   product_category  \n",
       "0         furniture  \n",
       "1  home_improvement  \n",
       "2              home  \n",
       "3          wireless  \n",
       "4                pc  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "train = pd.read_csv(\"../../data/train.csv\")\n",
    "test = pd.read_csv(\"../../data/test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acecf06-ade3-47fd-829f-cb14f58500e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: a helper function to absract the \"fit + predict + score\" logic.\n",
    "from sklearn import metrics\n",
    "\n",
    "def fit_and_score(clf, train_x, train_y, test_x, test_y):\n",
    "    \"\"\"fit the model `clf` to the `train` dataset and evaluate its\n",
    "    performance on the `test` dataset.\"\"\"\n",
    "    clf.fit(train_x, train_y)\n",
    "    preds = clf.predict(test_x)\n",
    "    \n",
    "    # calculate some classification metrics\n",
    "    accuracy = metrics.accuracy_score(preds, test_y)\n",
    "    f1 = metrics.f1_score(preds, test_y, average=\"macro\")\n",
    "\n",
    "    # and some regression metrics (since \"predict the number of stars\"\n",
    "    # could reasonably be either kind of task).\n",
    "    r2 = metrics.r2_score(preds, test_y)\n",
    "    mae = metrics.mean_absolute_error(preds, test_y)\n",
    "    \n",
    "    return pd.Series({\"Accuracy\": accuracy, \"F1\": f1, \"R2\": r2, \"MAE\": mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901f629-b1fb-4b37-ab21-a1525483da18",
   "metadata": {},
   "source": [
    "`spacy` is build arounds its downloadable models, which have been trained by the developers to do a wide range of linguistic annotation tasks like tokenization, part-of-speech tagging, syntactic dependency analysis, and more.  You do have to download the models before you can use them, using the command `python -m spacy download [model]` from the command line.  (and from inside your conda environment/virtual environment/whatever sort of environment you migth be using).  But once it's downloaded, it's super easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b809966-a240-4b27-8f7e-f0f828a10a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# the small English model--optimized for speed and memory footprint,\n",
    "# but at the cost of (a little bit of) accuracy.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# run the full annotation pipeline on a piece of text.\n",
    "doc = nlp(\"This is an example sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6191fe2-3b3b-4df9-9adf-c3fa08ca0945",
   "metadata": {},
   "source": [
    "`doc` now stores the _document_ after `spacy` processes it.  `doc` behaves like a list of _annotated tokens_.  Note that token annotations are accessible via attributes, and most attributes have two versions: with with an underscore at the end (e.g.: `token.lemma_`) and one without (`token.lemma`).  You usually want the underscore version.  The non-underscore version returns an integer value, which `spacy` uses internally to track and work with tokens.  (It's actually faster for `spacy` to work with numeric representations rather than strings).  You should basically never need the non-underscore-having attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056b8559-bb40-43d8-a966-bab1d0637110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This this this PRON DT O nsubj True Number=Sing|PronType=Dem\n",
      "is is be AUX VBZ O ROOT True Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "an an an DET DT O det True Definite=Ind|PronType=Art\n",
      "example example example NOUN NN O compound False Number=Sing\n",
      "sentence sentence sentence NOUN NN O attr False Number=Sing\n",
      ". . . PUNCT . O punct False PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(\n",
    "        token,          # original orthographic form of the token\n",
    "        token.lower_,   # lowercased version of the token\n",
    "        token.lemma_,   # lemmatized (stemmed) form of the token\n",
    "        token.pos_,     # coarse-grained part of speech tag\n",
    "        token.tag_,     # fine-grained part of speech tag\n",
    "        token.ent_iob_, # named entity type\n",
    "        token.dep_,     # syntactic depdency role\n",
    "        token.is_stop,  # True if the token is a stopword, else False\n",
    "        token.morph,    # miscellaneous morphological information, in\n",
    "                        # `Feature=Value|Feature=Value|...` format.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312deb0f-e345-4fa3-9e55-c12f996129d2",
   "metadata": {},
   "source": [
    "We can use these annotations to filter tokens.  The function below keeps the lemmatized form of any word that isn't a stopword, isn't a punctuation token, isn't a whitespace token, and isn't a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98eb7a81-317e-4e9c-b6c6-c2a960bfe687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fc131e2e144ca5a768782c6843c7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef91ee5f14d746c1a6c2b416854527a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def spacy_preprocess(nlp, texts):\n",
    "    # nlp.pipe(docs) will run the pipeline over each document and return\n",
    "    # an iterator over processed documents.  This can be multiprocessed\n",
    "    # for extra speed.\n",
    "    docs = nlp.pipe(\n",
    "        tqdm(texts),\n",
    "        \n",
    "        # spaCy processes a few hundred documents per second at its\n",
    "        # default pipeline configuration.\n",
    "        # disable some steps we don't need to speed this up;\n",
    "        # \"parser\" = the syntactic parser, and \"ner\" = named entity\n",
    "        # recognition.\n",
    "        disable=[\"parser\", \"ner\"],\n",
    "        \n",
    "        # multiprocess it--but be warned, this creates a fully copy\n",
    "        # of the model in each worker process.\n",
    "        # this causes a good bit of startup overhead, but it's worth it\n",
    "        # for this much data.\n",
    "        n_process=8,\n",
    "        batch_size=500,\n",
    "    )\n",
    "\n",
    "    # we could filter the tokens in a lot of ways, but I'm choosing\n",
    "    # list comprehension today.\n",
    "    docs = (\n",
    "        [\n",
    "            tok.lemma_.lower()\n",
    "            for tok in doc\n",
    "            if not (\n",
    "                tok.is_stop     # no stopwords\n",
    "                or tok.is_space # no space tokens\n",
    "                or tok.is_punct # no punctuation tokens\n",
    "                or tok.is_digit # no numbers\n",
    "            )\n",
    "        ]\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "    # spacy has nothing like Gensim's `Dictionary`--so we'll join the tokens\n",
    "    # back into one string and feed it through scikit-learn's `CountVectorizer`.\n",
    "    docs = [\" \".join(i) for i in docs]\n",
    "    \n",
    "    return docs\n",
    "\n",
    "bow_train = spacy_preprocess(nlp, train[\"review_body\"])\n",
    "bow_test = spacy_preprocess(nlp, test[\"review_body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61867395-05c3-4e70-ad60-852752925c71",
   "metadata": {},
   "source": [
    "Note the speed--this is a lot slower than `gensim`, even when we use multiprocessing for a speedup.  But this isn't surprising, since the `spacy` models are doing a lot more work than the `gensim` preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f186d214-424e-4910-9d2e-fdff00fe0d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awful fabric feel like tablecloth fit like child clothing customer service nice regret miss return date donate quality poor'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57876232-e538-4a39-a1a2-5a7281630c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy    0.486365\n",
       "F1          0.468694\n",
       "R2          0.348594\n",
       "MAE         0.809565\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit, predict, and score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline([\n",
    "    (\"bag of words\", CountVectorizer(max_df=0.5, min_df=10)),\n",
    "    (\"clf\", BernoulliNB())\n",
    "])\n",
    "fit_and_score(\n",
    "    clf,\n",
    "    bow_train,\n",
    "    train[\"stars\"],\n",
    "    bow_train,\n",
    "    train[\"stars\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f0b04-9c1a-4e55-aabd-7b1250c94887",
   "metadata": {},
   "source": [
    "There is a _lot_ more you can do with spaCy.  It excels at anything where you need to have linguistically-relevant annotations (e.g.: grammatical and semantic annotations; the \"mechanisms of language itself\" rather than \"thing that language might be connected to\"), but it does a _lot_ of processing, so you will often be trading speed for the extra accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
