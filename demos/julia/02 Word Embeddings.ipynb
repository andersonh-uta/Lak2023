{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ef0281-0be0-4716-b0c0-921d17cda3ec",
   "metadata": {},
   "source": [
    "# Word Embeddings in Julia\n",
    "\n",
    "In Julia, the word embedding approach requires engaging a bit more directly with the underlying mechanisms--but this doesn't really add any complexity, nor does it really compromise on speed at all.  The approach we'll take is as follows:\n",
    "- Load and tokenize our data.\n",
    "- Load some pre-trained word vectors (in the first section), and train our own (in the second).\n",
    "- For each token in our data, look up the word vectors, and represent each document as the sum of these vectors.\n",
    "- Throw a small neural network at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2a4b0a-c556-443f-92e7-3138546d88b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `C:\\Users\\andersonh\\Documents\\UA Projects\\LAK 2023\\demos\\julia`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ded94c-93f2-4830-a31a-78ccf7144393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "# Pkg.add(\"CSV\")\n",
    "# Pkg.add(\"CUDA\")\n",
    "# Pkg.add(\"DataFrames\")\n",
    "# Pkg.add(\"Embeddings\")\n",
    "# Pkg.add(\"Flux\")\n",
    "# Pkg.add(\"Pipe\")\n",
    "# Pkg.add(\"ProgressMeter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c736ef38-013e-4df1-a271-0927468e2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "using CSV\n",
    "using DataFrames\n",
    "\n",
    "train = DataFrame(CSV.File(\"../../data/train.csv\"))\n",
    "test = DataFrame(CSV.File(\"../../data/test.csv\"))\n",
    "val = DataFrame(CSV.File(\"../../data/validation.csv\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160eafe2-6b34-4aed-acda-70bd5b107058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mPreprocessing training data 100%|████████████████████████| Time: 0:00:03\u001b[39mm39m\n",
      "\u001b[32mPreprocessing testing data 100%|█████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81-element Vector{String}:\n",
       " \"arrived\"\n",
       " \"broken\"\n",
       " \"manufacturer\"\n",
       " \"defect\"\n",
       " \"two\"\n",
       " \"the\"\n",
       " \"legs\"\n",
       " \"the\"\n",
       " \"base\"\n",
       " \"were\"\n",
       " \"not\"\n",
       " \"completely\"\n",
       " \"formed\"\n",
       " ⋮\n",
       " \"there\"\n",
       " \"aren\"\n",
       " \"missing\"\n",
       " \"structures\"\n",
       " \"and\"\n",
       " \"supports\"\n",
       " \"that\"\n",
       " \"don\"\n",
       " \"impede\"\n",
       " \"the\"\n",
       " \"assembly\"\n",
       " \"process\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pipe # run `Pkg.add(\"Pipe\")` if needed\n",
    "using ProgressMeter\n",
    "\n",
    "# very similar preprocessing to before, but without stemming\n",
    "preprocess(s::String) :: Vector{String} = @pipe (\n",
    "    s\n",
    "    |> lowercase(_)\n",
    "    |> replace(_, r\"[^a-z]+\" => \" \")\n",
    "    |> split(_)\n",
    "    |> filter(x -> length(x) >= 3, _)\n",
    ")\n",
    "\n",
    "train_tokens = @showprogress \"Preprocessing training data\" [\n",
    "     preprocess(i) for i in train[!, :review_body]\n",
    "]\n",
    "test_tokens = @showprogress \"Preprocessing testing data\" [\n",
    "     preprocess(i) for i in test[!, :review_body]\n",
    "]\n",
    "val_tokens = @showprogress \"Preprocessing testing data\" [\n",
    "     preprocess(i) for i in val[!, :review_body]\n",
    "]\n",
    "\n",
    "train_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5418ff5-ea5c-437b-abfe-7edcf45c13a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# find and remove tokens that occur < 5 times total\n",
    "function counter(it)\n",
    "    counts = Dict()\n",
    "    for i ∈ it\n",
    "        counts[i] = get(counts, i, 0) + 1\n",
    "    end\n",
    "    return counts\n",
    "end\n",
    "\n",
    "whitelist = counter([tok for doc in train_tokens for tok in doc])\n",
    "whitelist = Set(tok for (tok, count) ∈ whitelist if count > 5)\n",
    "\n",
    "train_tokens = @showprogress [[tok for tok ∈ doc if tok ∈ whitelist] for doc in train_tokens]\n",
    "test_tokens = @showprogress [[tok for tok ∈ doc if tok ∈ whitelist] for doc in test_tokens]\n",
    "val_tokens = @showprogress [[tok for tok ∈ doc if tok ∈ whitelist] for doc in val_tokens];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9952730-939d-41f3-a107-5aefab526517",
   "metadata": {},
   "source": [
    "# Load pre-trained word vectors with `Embeddings.jl`\n",
    "\n",
    "`Embeddings.jl` provides a nice, simple interface to pre-trained word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d06907b-dc97-48cb-9caf-9bf5d07533ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "# Pkg.add(\"Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9e3eb7-0a85-4dc5-9603-ec43155d2a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings.EmbeddingTable{Matrix{Float32}, Vector{String}}(Float32[0.04656 -0.25539 … 0.81451 0.429191; 0.21318 -0.25723 … -0.36221 -0.296897; … ; -0.20989 -0.12226 … 0.28408 0.32618; 0.053913 0.35499 … -0.17559 -0.0590532], [\"the\", \",\", \".\", \"of\", \"to\", \"and\", \"in\", \"a\", \"\\\"\", \"'s\"  …  \"sigarms\", \"katuna\", \"aqm\", \"1.3775\", \"corythosaurus\", \"chanty\", \"kronik\", \"rolonda\", \"zsombor\", \"sandberger\"])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Embeddings\n",
    "\n",
    "# downloads the vector files if needes\n",
    "# The \"4\" specifies which of the Glove embeddings to load--this loads\n",
    "# the 300-dimensional ones.  Check the Embeddings.jl documentation for\n",
    "# more information.\n",
    "vectors = load_embeddings(GloVe{:en}, 4)\n",
    "\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6ab60-1b4a-4374-9e94-b91cefec1d2a",
   "metadata": {},
   "source": [
    "The `EmbeddingTable` is a struct with two fields:\n",
    "- `embeddings`: the table with one column per word, and one row per embedding dimension.\n",
    "- `vocab`: a `Vector` of string names.  The $i^{th}$ string's vector is the $i^{th}$ row in the `embeddings` array.\n",
    "\n",
    "We need to add one little mapping to convert a word into its corresponding vector.  Note that `Flux.jl`--which we'll use to build our small neural network--expects _one row per feature, one column per observation_, since Julia uses column-major ordering for arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f8616e-2792-4d3a-99ca-ccfca24348e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300-element Vector{Float32}:\n",
       "  0.59205\n",
       "  0.5055\n",
       " -0.19275\n",
       " -0.83702\n",
       " -0.20503\n",
       " -0.3296\n",
       " -0.20368\n",
       " -0.085202\n",
       " -0.27045\n",
       " -1.3407\n",
       "  0.16294\n",
       " -0.37931\n",
       "  0.30412\n",
       "  ⋮\n",
       " -0.38281\n",
       "  0.20347\n",
       "  0.1666\n",
       " -0.25304\n",
       "  0.33967\n",
       " -0.012803\n",
       " -0.11522\n",
       "  0.63322\n",
       " -0.026877\n",
       "  0.17706\n",
       "  0.23072\n",
       "  0.15622"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_TO_IDX = Dict(reverse.(enumerate(vectors.vocab)))\n",
    "\n",
    "function get_vector(word, tok2id, embeddings)\n",
    "    if !(word ∈ keys(tok2id))\n",
    "        return zeros(size(embeddings.embeddings)[1])\n",
    "    else\n",
    "        return embeddings.embeddings[:, tok2id[word]]\n",
    "    end\n",
    "end\n",
    "\n",
    "get_vector(\"manufacturer\", WORD_TO_IDX, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37e2d8c4-755b-421a-b81b-921d956c3ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:12\u001b[39mm\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "function doc2vec(doc, tok2id, embeddings)\n",
    "    if length(doc) == 0\n",
    "        return zeros(size(embeddings.embeddings)[1])\n",
    "    end\n",
    "    return @pipe (\n",
    "        [get_vector(tok, tok2id, embeddings) for tok ∈ doc]\n",
    "        |> reduce(hcat, _)\n",
    "        |> sum(_, dims=2) ./ size(_)[2]\n",
    "    )\n",
    "end\n",
    "\n",
    "train_vectors = reduce(\n",
    "    hcat,\n",
    "    @showprogress [doc2vec(i, WORD_TO_IDX, vectors) for i ∈ train_tokens]\n",
    ")\n",
    "test_vectors = reduce(\n",
    "    hcat,\n",
    "    @showprogress [doc2vec(i, WORD_TO_IDX, vectors) for i ∈ test_tokens]\n",
    ")\n",
    "val_vectors = reduce(\n",
    "    hcat,\n",
    "    @showprogress [doc2vec(i, WORD_TO_IDX, vectors) for i ∈ val_tokens]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff87306-7969-4079-a584-800d85616462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode our y-values for cross-entropy loss\n",
    "function one_hot(labels)\n",
    "    encoded = zeros(length(unique(labels)), size(labels)[1])\n",
    "    for l ∈ 1:length(labels)\n",
    "        encoded[labels[l], l] = 1\n",
    "    end   \n",
    "    return encoded\n",
    "end\n",
    "train_y = one_hot(train[!, :stars])\n",
    "test_y = one_hot(test[!, :stars])\n",
    "val_y = one_hot(val[!, :stars]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47fa00-87ad-42dc-9114-8b44edcb771b",
   "metadata": {},
   "source": [
    "Now let's throw a small neural network at it using the `Flux.jl` library, which is (currently) Julia's primary neural network library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b2bc68-ac41-4f25-8150-2a3bbda9ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install if needed\n",
    "# Pkg.add(\"Flux\")\n",
    "# Pkg.add(\"CUDA\") # if you have a CUDA-compatible GPU\n",
    "using Flux\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dec4e33f-e0d8-432c-bfc3-747940d7d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our data into DataLoaders, which wrap the batching logic for us.\n",
    "training_data = Flux.DataLoader(\n",
    "    (train_vectors, train_y) |> gpu,\n",
    "    batchsize=256,\n",
    "    shuffle=true,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcd5b66a-9d6e-40af-bf2e-49782ffee98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: val_loss=1.6120706644773484 acc=0.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch 1 training loop 100%|██████████████████████████████| Time: 0:00:47\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1: val_loss=1.238588478843961 acc=0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch 2 training loop 100%|██████████████████████████████| Time: 0:00:01\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 2: val_loss=1.2207526426566764 acc=0.4642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch 3 training loop 100%|██████████████████████████████| Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 3: val_loss=1.2161510502445512 acc=0.4668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch 4 training loop 100%|██████████████████████████████| Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 4: val_loss=1.2101987258592155 acc=0.4714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch 5 training loop 100%|██████████████████████████████| Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 5: val_loss=1.2141636575800134 acc=0.477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1968655926110106, 0.483)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our network\n",
    "model = Chain(\n",
    "    BatchNorm(size(vectors.embeddings)[1]),\n",
    "    Dense(size(vectors.embeddings)[1] => 256, relu),\n",
    "    Dense(256 => 256, relu),\n",
    "    Dense(256 => 256, relu),\n",
    "    Dense(256 => 5),\n",
    "    softmax,\n",
    ")\n",
    "model = gpu(model)\n",
    "\n",
    "# our optimizer\n",
    "optim = Flux.setup(Flux.Adam(0.01), model);\n",
    "\n",
    "# wrap the function evaluation logic\n",
    "function evaluate_model(model, x, y, loss_fn)\n",
    "    preds = cpu(model(gpu(x)))\n",
    "    hard_preds = [i.I[1] for i ∈ argmax(preds, dims=1)]\n",
    "    y_ = [i.I[1] for i ∈ argmax(y, dims=1)]\n",
    "    acc = sum(y_ .== hard_preds) / size(y)[2]\n",
    "    return loss_fn(preds, y), acc\n",
    "end\n",
    "    \n",
    "val_loss, acc = evaluate_model(model, val_vectors, val_y, Flux.crossentropy)\n",
    "println(\"Before training: val_loss=$val_loss acc=$acc\")\n",
    "for epoch in 1:5\n",
    "    @showprogress \"Epoch $epoch training loop\" for (x, y) in training_data\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            # Evaluate model and loss inside gradient context:\n",
    "            y_hat = m(x)\n",
    "            Flux.crossentropy(y_hat, y)\n",
    "        end\n",
    "        Flux.update!(optim, model, grads[1])\n",
    "    end\n",
    "    val_loss, acc = evaluate_model(model, val_vectors, val_y, Flux.crossentropy)\n",
    "    println(\"After epoch $epoch: val_loss=$val_loss acc=$acc\")\n",
    "end\n",
    "\n",
    "evaluate_model(model, test_vectors, test_y, Flux.crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2c9a4-5834-4c7d-bf2d-c20429c983b8",
   "metadata": {},
   "source": [
    "# Train your own word embeddings in Julia\n",
    "\n",
    "Sadly, there doesn't seem to be any good library for training your own word embeddings in Julia as of right now--but you can always train you own using Flux!  You could re-implement Word2Vec, or just one-hot encode your words and let the model learn task-specific embeddings.  Both of those require a lot more code than I'm going to show here, though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
